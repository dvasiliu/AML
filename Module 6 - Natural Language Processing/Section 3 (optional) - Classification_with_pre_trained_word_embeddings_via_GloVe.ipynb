{"cells":[{"cell_type":"markdown","source":["#<font size=8pt> Vector Embeddings"],"metadata":{"id":"P4AGOaC0V-gM"}},{"cell_type":"markdown","metadata":{"id":"RkA8T6AltEMI"},"source":["## <font color='blue'> Probabilistic Language Modeling\n","\n","**IMPORTANT** The conditional probability rule:\n","\n","$$\\large P(A\\cap B) = P(A) \\cdot P(B|A)$$\n","\n","**Goal:** Assign the probability that a sequence of words such as $(w_1,w_2,w_3,...w_n)$ occurs:\n","\n","$$\\text{P}(\\text{Sentence})=\\text{P}(w_1,w_2,w_3,...w_n)=\\text{P}(w_1)\\cdot\\text{P}(w_2,w_3,w_4...w_n|w_1)=$$\n","\n","Smartphones use this information to predict what the next word you will type will be, for example:\n","\n","$$\\text{P}(w_1,w_2,w_3,w_4)=\\text{P}(w_1)\\cdot\\text{P}(w_4,w_3,w_2|w_1)=\\text{P}(w_1)\\cdot\\text{P}(w_2|w_1)\\cdot\\text{P}(w_4,w_3|w_2,w_1)=\\text{P}(w_1)\\cdot\\text{P}(w_2|w_1)\\cdot\\text{P}(w_3|w_1,w_2)\\cdot\\text{P}(w_4|w_1,w_2,w_3)$$\n","\n","which mean the probability of word $w_4$ provided the words $w_1, w_2$ and $w_3$ occurred.\n","\n","<font face='Calibri' color='blue' size=4pt>Critical thinking:</span> How do we compute these probability values?</font>\n","\n","<span style=\"font-family:Calibri; color:red; font-size:14pt;\">Reasoning:</span> We compute the frequency of occurrence for different sequences of words.\n","\n","\n","<span style=\"font-family:Calibri; color:darkgreen; font-size:12pt;\"> P(today | It, is, sunny) = 50%\n","The model you use to predict is called the “language model” </span>\n","\n","\n","<span style=\"font-family:Calibri; color:purple; font-size:14pt;\"> Important Concept:</span> The Conditional Probability Rule states that probabilities of an events in the future are defined by the multiplication of all (conditional) probabilities leading to that given event.\n","\n","P(Today, it, was, sunny) = P(Today)  P(it | Today) P(was | Today, it)  P(sunny | Today, it, was)\n","\n","P(Today, is, the, fiftennth) = P(Today) P(is | Today) P(the | Today, is) P(fifteenth | Today, is, the)\n","\n","1. Unigram Models:\n","        a. P(rainy | Today, it, was) ~ P(Today) P(it) P(was)\n","2. Bigram Models:\n","        a. P(rainy| Today, it, was) ~ P(rainy | was)\n","3. N-gram models:\n","        a. Same as the above, but for arbitrary distances.\n","        b. For example a tri-gram: P(rainy | Today, it, was)\n","            \n","Often used in nested ways (i.e., a 3-gram model + unigram).\n","\n","\n","### Evaluating NLP\n","\n","• The goal of any NLP activity is important in deciding how to evaluate it.\n","\n","• In a Bag of Words model, evaluation can come from classification accuracy (i.e., you have a training and test dataset).\n","\n","• But what if you’re writing an algorithm that predicts the next word for a texting app?\n","\n","### Perplexity an evaluative measure for NLP\n","\n","One might expect a model to be good at predicting cold in this sentence:\n","\n","“It is cold.”\n","\n","And not as good at predicting:\n","\n","“It is very cool outside when the winter is cold”\n","\n","For a variety of reasons; the biggest is the complexity/length of the sentence.\n","\n","• Perplexity is a measurement of how well a probability model predicts a test data. In the context of Natural Language Processing, perplexity is one way to evaluate language models.\n","\n","• Perplexity is an exponentiation of the entropy.\n","\n","• Low perplexity is good and high perplexity is bad since the perplexity is the exponentiation of an \\cdotentropy\\cdot.\n","\n","• The goal is to minimize Perplexity(W).\n","\n","Calculation of perplexity for a full a sequence of words:\n","\n","$$\\sqrt[n]{\\prod_{i=1}^{n}\\frac{1}{P(w_i|w_{1}w_{2}...w_{i-1})}}$$\n","\n","Important applciations for Natural Language Processing:\n","\n","    • Sentiment Analysis\n","\n","    • Speech Recognition\n","\n","    • Information Retrieval\n","\n","    • Question Answering\n","\n","<span style=\"font-family:Calibri; color:blue; font-size:14pt;\">Big Idea:</span> Represent words as vectors: GloVe, Word2Vec algorithms (both are based on neural networks)."]},{"cell_type":"markdown","metadata":{"id":"KCVGm4mKJ3Kg"},"source":["## Global Vectors for Word Representations (GloVe)\n","\n","Reference: https://nlp.stanford.edu/projects/glove/\n","\n","Example for using the vector words:  <font color='red'>monarch - man = queen.</font>\n","\n","The main idea is that we can do more than just counting occurences but rather represent the words from the vocabulary of a language as vectors whose entries are real numbers. As such, the GloVe algorithm is analysing word \\cdotco-occurrencies\\cdot within a text corpus; the steps are as follows:\n","\n","1.   A *co-occurence* matrix $X$ is created where its entries $X_{ij}$ represent how often word $i$ is present in the context of the word $j$. Thus there is a parsing of the corpus for building the matrix $X$ and then the model is constructed based on this matrix.\n","2.   For the words $i$ and $j$ we create vectors $\\vec{w}_i$ and $\\vec{w}_j$ such that $$\\vec{w}_i^T\\cdot\\vec{w}_j+b_i+b_j=\\log (X_{ij})$$ where $b_i$ and $b_j$ are bias terms (i.e. intercept terms for a regression model). We want to build word vectors that retain useful information of how words $i$ and $j$ co-occur.\n","3.   In order to determine the entries for the $\\vec{w}_i$, we minimize the following objective function $$J:=\\sum_{i=1}^{V}\\sum_{j=1}^{V}f(X_{ij}) \\left(\\vec{w}_i^T\\cdot\\vec{w}_j+b_i+b_j-\\log (X_{ij})\\right)^2$$\n","4.   The function $f$ is chosen in order to prevent the skewing of the objective function by the words that co-occur too often. In this sense a choice for the function $f$ could be $$f(X_{ij}):=\\begin{cases}\n","\\left(\\frac{X_{ij}}{x_{max}}\\right)^{\\alpha} \\text{if} \\;\\; X_{ij}<x_{max} \\\\\n","1 \\;\\;\\; \\text{otherwise}\n","\\end{cases}\n","$$ where $\\alpha$ and $x_{max}$ can be adjusted by the user.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gdASXXg2siIt"},"source":["# Using pre-trained word embeddings\n","\n","Reference:\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2020/05/05<br>\n","**Last modified:** 2020/05/05<br>\n","**Description:** Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings."]},{"cell_type":"markdown","metadata":{"id":"D2NXtsAlsiIu"},"source":["## Setup"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"m3ELE4T3siIv","executionInfo":{"status":"ok","timestamp":1701960845715,"user_tz":300,"elapsed":399,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"aJaFaXTWsiIv"},"source":["## Introduction\n","\n","In this example, we show how to train a text classification model that uses pre-trained\n","word embeddings.\n","\n","We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n","belonging to 20 different topic categories.\n","\n","For the pre-trained word embeddings, we'll use\n","[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."]},{"cell_type":"markdown","metadata":{"id":"9nvdxNwNsiIw"},"source":["## Download the Newsgroup20 data"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"yhr5bl8jsiIw","executionInfo":{"status":"ok","timestamp":1701960889336,"user_tz":300,"elapsed":4918,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["data_path = keras.utils.get_file(\n","    \"news20.tar.gz\",\n","    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n","    untar=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"nLXfQDxnsiIw"},"source":["## Let's take a look at the data"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"ag9UVvO3siIw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1472cc2-1b03-4c1d-c7af-3bfafa6997ed","executionInfo":{"status":"ok","timestamp":1701960936043,"user_tz":300,"elapsed":528,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of directories: 20\n","Directory names: ['comp.os.ms-windows.misc', 'alt.atheism', 'comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'sci.electronics', 'comp.sys.mac.hardware', 'talk.politics.misc', 'talk.politics.mideast', 'talk.politics.guns', 'talk.religion.misc', 'rec.sport.hockey', 'sci.space', 'sci.crypt', 'soc.religion.christian', 'misc.forsale', 'rec.autos', 'sci.med', 'comp.windows.x', 'rec.motorcycles', 'comp.graphics']\n","Number of files in rec.motorcycles: 1000\n","Some example filenames: ['104343', '104615', '104653', '103170', '104487', '104755', '104767', '104802', '104519', '104359', '105214', '104765', '104435', '104870', '105064']\n"]}],"source":["import os\n","import pathlib\n","\n","data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n","dirnames = os.listdir(data_dir)\n","print(\"Number of directories:\", len(dirnames))\n","print(\"Directory names:\", dirnames)\n","\n","fnames = os.listdir(data_dir / \"rec.motorcycles\")\n","print(\"Number of files in rec.motorcycles:\", len(fnames))\n","print(\"Some example filenames:\", fnames[:15])"]},{"cell_type":"markdown","metadata":{"id":"aw8oQRkzsiIx"},"source":["Here's a example of what one file contains:"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"lXFwFKlUsiIx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3d0ce7e-5b1f-402c-f288-4ae7a0129f18","executionInfo":{"status":"ok","timestamp":1701961001477,"user_tz":300,"elapsed":546,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Newsgroups: rec.motorcycles\n","Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!howland.reston.ans.net!agate!doc.ic.ac.uk!syma!tafi3\n","From: tafi3@syma.sussex.ac.uk (Ian Deeley)\n","Subject: Re: CB750 C with flames out the exhaust!!!!---->>>\n","Message-ID: <1993Apr20.133712.10909@syma.sussex.ac.uk>\n","Organization: University of Sussex\n","References: <C5quw0.Btq@ux1.cso.uiuc.edu>\n","Date: Tue, 20 Apr 1993 13:37:12 GMT\n","Lines: 25\n","\n","From article <C5quw0.Btq@ux1.cso.uiuc.edu>, by mikeh@ux1.cso.uiuc.edu (Mike Hollyman):\n","> Hi, I have an 82 CB750 Custom that I just replaced the cylinder head gasket\n","> on.  Now when I put it back together again, it wouldn't idle at all.  It was\n","> only running on 2-3 cylinders and it would backfire and spit flames out the\n","> exhaust on the right side.  The exhaust is 4-2 MAC.  I bought new plugs\n","> today and it runs very rough and still won't idle.  I am quite sure the fine\n","> tune knobs on the carbs are messed up.  I checked the timing, it was fine, so\n","> I advanced it a little and that didn't help.  \n","> \n","> I assume the carbs need to be synched.  Can I buy a kit and do this myself?\n","> If so, what kit is the best for the price.\n","> \n","> Any other suggestions?\n","> \n","\tI dont think its the carbs that are out, I would suspect that\n","the cam timing is out, & as you say that you had the head off, that\n","would make sense to me,\n","\t\t\t\tIan.\n","\n","\tJust my 0.02 emu's worth.\n","-- \n","Ian Deeley \t\t\t\t\"...Whatever you do will be\n","School of Engineering\t   | |\t\tinsignificant, but its very\t\n","University of Sussex   \t--=oOo=--\timportant that you do it..\"\n","England.\t\t\t\t\tAnon\n","\n"]}],"source":["print(open(data_dir / \"rec.motorcycles\" / \"104521\").read())"]},{"cell_type":"markdown","metadata":{"id":"FwZp6hSdsiIx"},"source":["As you can see, there are header lines that are leaking the file's category, either\n","explicitly (the first line is literally the category name), or implicitly, e.g. via the\n","`Organization` filed. Let's get rid of the headers:"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"6Oyx9BtHsiIx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddab1460-649f-46fb-d35c-9937eddd10c8","executionInfo":{"status":"ok","timestamp":1701961096952,"user_tz":300,"elapsed":1091,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing alt.atheism, 1000 files found\n","Processing comp.graphics, 1000 files found\n","Processing comp.os.ms-windows.misc, 1000 files found\n","Processing comp.sys.ibm.pc.hardware, 1000 files found\n","Processing comp.sys.mac.hardware, 1000 files found\n","Processing comp.windows.x, 1000 files found\n","Processing misc.forsale, 1000 files found\n","Processing rec.autos, 1000 files found\n","Processing rec.motorcycles, 1000 files found\n","Processing rec.sport.baseball, 1000 files found\n","Processing rec.sport.hockey, 1000 files found\n","Processing sci.crypt, 1000 files found\n","Processing sci.electronics, 1000 files found\n","Processing sci.med, 1000 files found\n","Processing sci.space, 1000 files found\n","Processing soc.religion.christian, 997 files found\n","Processing talk.politics.guns, 1000 files found\n","Processing talk.politics.mideast, 1000 files found\n","Processing talk.politics.misc, 1000 files found\n","Processing talk.religion.misc, 1000 files found\n","Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n","Number of samples: 19997\n"]}],"source":["# automated pre-processing of Newsgroups Dataset\n","samples = []\n","labels = []\n","class_names = []\n","class_index = 0\n","for dirname in sorted(os.listdir(data_dir)):\n","    class_names.append(dirname)\n","    dirpath = data_dir / dirname\n","    fnames = os.listdir(dirpath)\n","    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n","    for fname in fnames:\n","        fpath = dirpath / fname\n","        f = open(fpath, encoding=\"latin-1\")\n","        content = f.read()\n","        lines = content.split(\"\\n\")\n","        lines = lines[10:]\n","        content = \"\\n\".join(lines)\n","        samples.append(content)\n","        labels.append(class_index)\n","    class_index += 1\n","\n","print(\"Classes:\", class_names)\n","print(\"Number of samples:\", len(samples))"]},{"cell_type":"code","source":["len(samples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDmJTsBDIehz","outputId":"d4f0781e-a51a-4f8a-92a8-f17cea5bd235","executionInfo":{"status":"ok","timestamp":1701955853205,"user_tz":300,"elapsed":616,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19997"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"ZXICksFnsiIy"},"source":["There's actually one category that doesn't have the expected number of files, but the\n","difference is small enough that the problem remains a balanced classification problem."]},{"cell_type":"markdown","metadata":{"id":"HBbhrugJsiIy"},"source":["## Shuffle and split the data into training & validation sets"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"ajA5RxNEsiIy","executionInfo":{"status":"ok","timestamp":1701961199613,"user_tz":300,"elapsed":390,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["# Shuffle the data\n","seed = 1234\n","rng = np.random.RandomState(seed)\n","rng.shuffle(samples)\n","rng = np.random.RandomState(seed)\n","rng.shuffle(labels)\n","\n","# Extract a training & validation split\n","validation_split = 0.25\n","num_validation_samples = int(validation_split * len(samples))\n","train_samples = samples[:-num_validation_samples]\n","val_samples = samples[-num_validation_samples:]\n","train_labels = labels[:-num_validation_samples]\n","val_labels = labels[-num_validation_samples:]"]},{"cell_type":"code","source":["train_samples[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"ToXbN6tgTCCa","outputId":"a6cb052f-bccc-42f7-8af1-01002534c68c","executionInfo":{"status":"ok","timestamp":1701961231365,"user_tz":300,"elapsed":328,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"NNTP-Posting-Host: piaget.phys.ksu.edu\\n\\nIn <C5qGF5.K2I@alta-oh.com> chris@zeus.alta-oh.com (Chris Murphy) writes:\\n\\n>In article <FULL_GL.93Apr18005752@dolphin.pts.mot.com>, full_gl@pts.mot.com (Glen Fullmer) writes:\\n>|> Looking for a graphics/CAD/or-whatever package on a X-Unix box that will\\n>|> take a file with records like:\\n\\n>Hi,\\n>  See Roger Grywalski's response to :\\n\\n>Re: Help on network visualization\\n\\n>in comp.graphics.visualization.\\n\\nCould someone please post Roger Grywalski's response?  Or point me to where\\nI could find it?\\n\\nThanks a lot,\\n\\n\\nS. Raj Chaudhury\\t\\t\\t|\\nDept. of Physics    \\t\\t\\t|  raj@phys.ksu.edu\\nKansas State University\\t\\t\\t|\\nManhattan, KS 66506\\t\\t\\t|\\n--\\nS. Raj Chaudhury\\t\\t\\t|\\nDept. of Physics    \\t\\t\\t|  raj@phys.ksu.edu\\nKansas State University\\t\\t\\t|\\nManhattan, KS 66506\\t\\t\\t|\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"vVmzajaasiIy"},"source":["## Create a vocabulary index\n","\n","Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n","Later, we'll use the same layer instance to vectorize the samples.\n","\n","Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n","be actually 200 tokens long."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"YR-GKQ77siIy","executionInfo":{"status":"ok","timestamp":1701961489400,"user_tz":300,"elapsed":2355,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["from tensorflow.keras.layers import TextVectorization\n","\n","vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=500)\n","text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n","vectorizer.adapt(text_ds)"]},{"cell_type":"markdown","metadata":{"id":"H6pOC96FsiIz"},"source":["You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n","print the top 5 words:"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"0KPLl7UJsiIz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701961515071,"user_tz":300,"elapsed":641,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}},"outputId":"7954f5ac-39c5-451b-b80c-585d22599280"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'the',\n"," 'to',\n"," 'of',\n"," 'a',\n"," 'and',\n"," 'in',\n"," 'is',\n"," 'i',\n"," 'that',\n"," 'it',\n"," 'for',\n"," 'you',\n"," 'this',\n"," 'on',\n"," 'be',\n"," 'not',\n"," 'are',\n"," 'have',\n"," 'with',\n"," 'as',\n"," 'or',\n"," 'if',\n"," 'but',\n"," 'was',\n"," 'they',\n"," 'from',\n"," 'by',\n"," 'at',\n"," 'an',\n"," 'can',\n"," 'what',\n"," 'my',\n"," 'would',\n"," 'all',\n"," 'there',\n"," 'will',\n"," 'one',\n"," 'writes',\n"," 'do',\n"," 'about',\n"," 'we',\n"," 'so',\n"," 'your',\n"," 'has',\n"," 'he',\n"," 'article',\n"," 'no',\n"," 'any',\n"," 'me',\n"," 'some',\n"," 'who',\n"," 'which',\n"," 'its',\n"," 'were',\n"," 'dont',\n"," 'out',\n"," 'people',\n"," 'when',\n"," 'like',\n"," 'just',\n"," 'more',\n"," 'their',\n"," '1',\n"," 'know',\n"," 'other',\n"," 'only',\n"," 'them',\n"," 'up',\n"," 'get',\n"," 'how',\n"," 'than',\n"," 'had',\n"," 'lines',\n"," 'been',\n"," 'think',\n"," '2',\n"," 'his',\n"," 'also',\n"," 'does',\n"," 'then',\n"," 'use',\n"," 'time',\n"," 'these',\n"," 'im',\n"," 'should',\n"," 'could',\n"," 'well',\n"," 'may',\n"," 'good',\n"," 'because',\n"," 'us',\n"," 'even',\n"," 'am',\n"," 'now',\n"," 'new',\n"," 'see',\n"," 'very',\n"," 'into']"]},"metadata":{},"execution_count":47}],"source":["vectorizer.get_vocabulary()[:100]"]},{"cell_type":"markdown","metadata":{"id":"WcVnvqGdsiIz"},"source":["Let's vectorize a test sentence:"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"HpFoNoycsiIz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8fc3b78-37eb-4758-9105-5ff329deb82e","executionInfo":{"status":"ok","timestamp":1701961654297,"user_tz":300,"elapsed":327,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  14,    8,  198, 4692,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0])"]},"metadata":{},"execution_count":48}],"source":["output = vectorizer([[\"This is Data 301.\"]])\n","output.numpy()[0, :]"]},{"cell_type":"code","source":["len(np.array(output)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Ux4aQFQ0wYf","executionInfo":{"status":"ok","timestamp":1701961845013,"user_tz":300,"elapsed":7,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}},"outputId":"05218177-658c-4af4-ecc5-ae947cfaa32c"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"nOOCUx6bsiIz"},"source":["As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n","word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n","reserved for \"out of vocabulary\" tokens.\n","\n","Here's a dict mapping words to their indices:"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"d_QeibQTsiIz","executionInfo":{"status":"ok","timestamp":1701961910599,"user_tz":300,"elapsed":421,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["voc = vectorizer.get_vocabulary()\n","word_index = dict(zip(voc, range(len(voc))))"]},{"cell_type":"code","source":["len(voc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOQWfqOsK1y9","outputId":"09cdd1b9-4df3-486a-bbb3-3e4efaad677c","executionInfo":{"status":"ok","timestamp":1701961914087,"user_tz":300,"elapsed":329,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30000"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"jV4qEmQVsiIz"},"source":["As you can see, we obtain the same encoding as above for our test sentence:"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"2TOEplqgsiIz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"962aede1-8358-412e-c18b-719dbbe3a190","executionInfo":{"status":"ok","timestamp":1701961943572,"user_tz":300,"elapsed":311,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3718, 6557, 15, 2, 4214]"]},"metadata":{},"execution_count":60}],"source":["test = [\"the\", \"cat\", \"jumped\", \"on\", \"the\", \"hat\"]\n","[word_index[w] for w in test]"]},{"cell_type":"markdown","metadata":{"id":"YM_j4sfKsiIz"},"source":["## Load pre-trained word embeddings"]},{"cell_type":"markdown","metadata":{"id":"z2K5LrFHsiIz"},"source":["Let's download pre-trained GloVe embeddings (a 822M zip file).\n","\n","You'll need to run the following commands:\n","\n","```\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip\n","```"]},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruqrgwZ2CTT0","outputId":"9b4f5661-8b1a-42de-a655-c8f3143129f4","executionInfo":{"status":"ok","timestamp":1701952138140,"user_tz":300,"elapsed":182076,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-07 12:25:55--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-12-07 12:25:56--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-12-07 12:25:56--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2023-12-07 12:28:36 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"cJydt4nPsiIz"},"source":["The archive contains text-encoded vectors of various sizes: 50-dimensional,\n","100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n","\n","Let's make a dict mapping words (strings) to their NumPy vector representation:"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"0hCkaEJJsiIz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8aaae17-54b5-4af2-f327-62ee5d50b42c","executionInfo":{"status":"ok","timestamp":1701962169624,"user_tz":300,"elapsed":21146,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n"]}],"source":["path_to_glove_file = os.path.join(\"glove.6B.300d.txt\")\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))"]},{"cell_type":"code","source":["vu = embeddings_index['love']\n","vw = embeddings_index['friendship']"],"metadata":{"id":"9iePs-yPDwFX","executionInfo":{"status":"ok","timestamp":1701962475738,"user_tz":300,"elapsed":332,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["vu"],"metadata":{"id":"ajedH7qoCW0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701962228957,"user_tz":300,"elapsed":402,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}},"outputId":"6a25f746-24d4-4254-b243-216687e0f73e"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4.6443e-01,  3.7730e-01, -2.1459e-01, -5.0768e-01, -2.4576e-01,\n","        8.1340e-02,  1.0145e-01,  2.5155e-01, -3.6152e-01, -1.6030e+00,\n","        2.8219e-01,  3.6653e-01,  4.4611e-01,  2.7950e-01,  4.7722e-02,\n","        3.0087e-01, -1.6226e-01, -2.6055e-02, -2.6815e-01, -4.6282e-01,\n","        2.5012e-01,  6.0389e-01,  1.5111e-01, -6.2823e-02, -9.6755e-02,\n","       -3.0548e-01, -1.1376e-01,  5.3914e-01,  1.0966e-01, -7.0618e-01,\n","       -6.6316e-01,  4.3559e-01, -4.8631e-02,  2.7755e-01, -4.8685e-01,\n","        1.1938e-01, -5.4538e-01, -2.9563e-01,  3.4470e-02,  5.3187e-01,\n","       -1.5880e-03,  4.1692e-01, -2.0742e-01, -3.7833e-02,  4.3333e-01,\n","        4.7521e-02,  8.3507e-01, -6.5088e-02, -2.9974e-01,  4.7139e-03,\n","        1.2339e-01, -5.0660e-01,  2.5870e-01,  2.1264e-01,  1.9132e-01,\n","        5.4204e-01, -1.1385e-01, -4.2384e-01, -2.7808e-01, -1.5105e-01,\n","       -6.2104e-01,  2.7678e-01, -5.4974e-02,  1.8479e-02, -1.1744e-01,\n","        3.3029e-01, -3.5251e-01, -2.1953e-01,  5.5140e-02,  1.6971e-01,\n","       -3.6445e-01,  3.6383e-01,  2.0496e-01,  6.6512e-01, -2.2540e-01,\n","       -3.0347e-01, -5.2782e-01, -6.6115e-01,  1.1774e-01, -1.4438e-01,\n","        3.3886e-01,  2.2970e-01,  6.3072e-01,  7.6929e-01,  2.5360e-02,\n","       -1.3685e-01,  8.4254e-02,  3.2036e-01, -1.0431e-01,  7.6720e-01,\n","        1.0674e+00,  4.2885e-01,  3.0320e-02, -6.3375e-01,  7.0705e-01,\n","       -1.7671e-01, -1.0770e-01, -3.1877e-01,  6.3781e-02, -5.7507e-01,\n","       -4.0656e-01,  5.5837e-01,  7.2376e-01,  2.6450e-01,  2.0313e-01,\n","       -4.3949e-01,  5.3094e-01,  7.5492e-02, -2.6951e-01, -4.2770e-01,\n","       -1.3859e-01,  2.8341e-01,  1.3153e-01, -3.2371e-01,  2.2816e-01,\n","        4.3016e-01, -1.5529e-02,  1.3276e-01,  4.8239e-02, -3.8536e-02,\n","        1.3813e-02, -2.4567e-01,  4.6580e-01, -6.1963e-01, -3.9568e-01,\n","       -2.1261e-01,  1.1121e-01, -1.9519e-01,  3.2773e-01, -2.5276e-01,\n","        8.7512e-01,  4.9860e-01,  2.5747e-01,  4.6459e-01, -9.7237e-02,\n","        4.2509e-01,  8.0811e-03, -7.9669e-01,  1.9793e-01, -6.0157e-02,\n","        6.0480e-02,  2.0092e-01,  4.6373e-01,  2.9617e-01, -3.1812e-01,\n","        1.2548e-01, -1.7667e-01, -2.9347e-01, -5.9183e-01,  4.3081e-01,\n","        1.0006e+00,  4.6858e-01,  1.1257e-01, -5.6533e-01,  5.4729e-01,\n","       -4.3424e-01,  2.2738e-01,  2.1527e-01,  8.7824e-02, -7.3562e-02,\n","        5.3160e-01, -3.6782e-01, -4.2531e-01, -6.6561e-02,  3.4648e-01,\n","       -1.3677e-01, -2.6654e-01,  9.9056e-02, -5.2121e-02, -6.1409e-01,\n","       -3.6171e-03,  3.2633e-01,  3.9050e-01,  2.9416e-02, -1.1190e-01,\n","       -2.1180e-01, -1.5688e-01, -2.9308e-01,  2.3114e-01,  1.3873e-01,\n","        6.6445e-01, -2.6318e-01, -2.5518e-01,  2.9499e-01,  7.0132e-01,\n","       -3.2390e-01,  4.2109e-01,  2.7316e-01,  1.5021e-01,  5.8074e-02,\n","        2.4201e-01, -6.2194e-01, -4.0832e-01,  3.8272e-02, -3.5372e-01,\n","        1.3934e-02, -2.5013e-01,  4.2916e-01, -3.9463e-02, -2.9794e-01,\n","        1.0178e+00, -4.1685e-01,  1.5442e-01,  1.9598e-01,  4.4718e-01,\n","       -2.5764e-01,  1.3438e-01, -8.4223e-02, -1.4142e-01, -1.6001e-01,\n","       -3.2541e-01, -1.9588e-01,  1.3934e-01, -4.4924e-01,  2.0827e-01,\n","       -1.5747e-01,  3.5537e-02,  3.7097e-01, -1.5336e-01,  2.0401e-01,\n","        3.5981e-01, -2.6311e-01, -2.7436e-01, -4.1229e-01,  4.5533e-01,\n","        7.7527e-01,  2.9339e-01, -2.7392e-01,  4.5037e-01,  6.0015e-02,\n","        8.1354e-02, -5.3969e-01,  3.1872e-01, -8.2837e-02,  7.7918e-01,\n","        9.4676e-02,  7.5256e-01,  3.3377e-01, -5.5349e-01, -1.5661e-01,\n","        2.0791e-01, -1.3498e-01,  4.7560e-01,  1.2180e-01, -1.6496e-01,\n","       -6.5669e-01,  5.2394e-01, -4.3666e-01, -8.7235e-03,  5.2565e-01,\n","       -1.2011e-01, -2.1406e-01, -1.5253e-01, -2.2775e-01,  5.9701e-01,\n","       -6.4509e-01, -4.4500e-01,  4.0937e-01,  1.1943e-01,  3.4467e-02,\n","       -3.2471e-01, -4.7915e-01, -2.5264e-01,  2.6818e-01,  5.9601e-02,\n","       -5.1842e-01,  3.3124e-01, -2.9623e-01, -2.8343e-01, -4.1471e-01,\n","       -4.5050e-02, -1.7776e-03, -3.7654e-01, -2.3911e-02,  5.5602e-01,\n","        2.0440e-01, -2.2391e+00,  1.4986e-01, -9.8345e-02,  1.8714e-01,\n","       -1.9265e-01, -6.1057e-03,  3.9897e-01, -2.2392e-01, -4.2168e-01,\n","        1.0727e+00,  2.9344e-01, -4.7384e-01,  3.0169e-01, -1.5307e-01,\n","        1.9966e-01, -4.0979e-01, -1.0325e-01, -4.3610e-02,  1.7564e-01,\n","        6.5709e-01, -9.9860e-02,  4.9107e-01,  2.8215e-01,  3.4554e-01],\n","      dtype=float32)"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["$$\\large <\\vec{u},\\vec{u}> = (u_1)^2 + (u_2)^2 + ... +(u_n)^2$$\n","\n","So the magnitude of $\\vec{u}$ is\n","\n","$$\\large \\|\\vec{u}\\| = \\sqrt{<\\vec{u},\\vec{u}>} $$"],"metadata":{"id":"PtsBLF8ANeP_"}},{"cell_type":"code","source":["vu.dot(vw)/(np.sqrt(vu.dot(vu))*np.sqrt(vw.dot(vw)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiX05So6NBpF","outputId":"fd5688f2-ba85-40f4-a7cd-aed8af6a2638","executionInfo":{"status":"ok","timestamp":1701962480202,"user_tz":300,"elapsed":333,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49185005"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["$$\\large <\\vec{u},\\vec{w}> = \\cos(\\theta)\\cdot \\|\\vec{u}\\|\\cdot \\|\\vec{w}\\|$$"],"metadata":{"id":"XWuGUODQ257H"}},{"cell_type":"code","source":[],"metadata":{"id":"mVORVt5-25SY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the actual angle between the two vector embeddings is\n","np.arccos(vu.dot(vw)/(np.sqrt(vu.dot(vu))*np.sqrt(vw.dot(vw))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mezee2H1Okdd","outputId":"50429c8c-3c1a-4af6-9ce0-922e7e930511"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0321491"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"p9YsLQ1NsiI0"},"source":["Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n","`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n","vector for the word of index `i` in our `vectorizer`'s vocabulary."]},{"cell_type":"code","execution_count":68,"metadata":{"id":"uaI9Xbc0siI0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9aa4284c-e7e2-42f8-eaeb-dc130f58b9b3","executionInfo":{"status":"ok","timestamp":1701962585518,"user_tz":300,"elapsed":908,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Converted 25083 words (4917 misses)\n"]}],"source":["num_tokens = len(voc) + 2\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","# Prepare embedding matrix\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros.\n","        # This includes the representation for \"padding\" and \"OOV\"\n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n"]},{"cell_type":"code","source":["embedding_matrix.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8S5B_4rLMLh","outputId":"f2ca1754-0c80-4591-bb81-599134f19b23","executionInfo":{"status":"ok","timestamp":1701956759100,"user_tz":300,"elapsed":350,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30002, 300)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["embedding_matrix[2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5W3i7iHUyhNz","executionInfo":{"status":"ok","timestamp":1701726270259,"user_tz":300,"elapsed":34,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}},"outputId":"310241ef-d3d8-4424-e26d-de9327ae4c25"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4.65600006e-02,  2.13180006e-01, -7.43639981e-03, -4.58539993e-01,\n","       -3.56389992e-02,  2.36430004e-01, -2.88360000e-01,  2.15210006e-01,\n","       -1.34859994e-01, -1.64129996e+00, -2.60910004e-01,  3.24340016e-02,\n","        5.66210002e-02, -4.32960019e-02, -2.16719992e-02,  2.24759996e-01,\n","       -7.51290023e-02, -6.70180023e-02, -1.42470002e-01,  3.88250016e-02,\n","       -1.89510003e-01,  2.99769998e-01,  3.93049985e-01,  1.78870007e-01,\n","       -1.73429996e-01, -2.11779997e-01,  2.36169994e-01, -6.36809990e-02,\n","       -4.23180014e-01, -1.16609998e-01,  9.37540010e-02,  1.72959998e-01,\n","       -3.30729991e-01,  4.91120011e-01, -6.89949989e-01, -9.24620032e-02,\n","        2.47419998e-01, -1.79910004e-01,  9.79079977e-02,  8.31179991e-02,\n","        1.52989998e-01, -2.72760004e-01, -3.89339998e-02,  5.44529974e-01,\n","        5.37370026e-01,  2.91049987e-01, -7.35139987e-03,  4.78800014e-02,\n","       -4.07599986e-01, -2.67590005e-02,  1.79189995e-01,  1.09770000e-02,\n","       -1.09630004e-01, -2.63949990e-01,  7.39900023e-02,  2.62360007e-01,\n","       -1.50800005e-01,  3.46230000e-01,  2.57580012e-01,  1.19709998e-01,\n","       -3.71350013e-02, -7.15930015e-02,  4.38980013e-01, -4.07640003e-02,\n","        1.64250005e-02, -4.46399987e-01,  1.71969995e-01,  4.62459996e-02,\n","        5.86390011e-02,  4.14990000e-02,  5.39479971e-01,  5.24950027e-01,\n","        1.13609999e-01, -4.83149998e-02, -3.63849998e-01,  1.87040001e-01,\n","        9.27610025e-02, -1.11290000e-01, -4.20850009e-01,  1.39919996e-01,\n","       -3.93379986e-01, -6.79450035e-02,  1.21880002e-01,  1.67070001e-01,\n","        7.51689970e-02, -1.55290002e-02, -1.94989994e-01,  1.96380004e-01,\n","        5.31940013e-02,  2.51700014e-01, -3.48450005e-01, -1.06380001e-01,\n","       -3.46920013e-01, -1.90239996e-01, -2.00399995e-01,  1.21540003e-01,\n","       -2.92079985e-01,  2.33529992e-02, -1.16180003e-01, -3.57679993e-01,\n","        6.23040013e-02,  3.58839989e-01,  2.90600006e-02,  7.30049983e-03,\n","        4.94820019e-03, -1.50480002e-01, -1.23130001e-01,  1.93370000e-01,\n","        1.21730000e-01,  4.45030004e-01,  2.51470000e-01,  1.07809998e-01,\n","       -1.77159995e-01,  3.86909992e-02,  8.15299973e-02,  1.46669999e-01,\n","        6.36660010e-02,  6.13319986e-02, -7.55689964e-02, -3.77240002e-01,\n","        1.58500001e-02, -3.03420007e-01,  2.83740014e-01, -4.20130007e-02,\n","       -4.07150015e-02, -1.52689993e-01,  7.49799982e-02,  1.55770004e-01,\n","        1.04330003e-01,  3.13930005e-01,  1.93090007e-01,  1.94289997e-01,\n","        1.51850000e-01, -1.01920001e-01, -1.87849998e-02,  2.07910001e-01,\n","        1.33660004e-01,  1.90380007e-01, -2.55580008e-01,  3.03999990e-01,\n","       -1.89599991e-02,  2.01470003e-01, -4.21099991e-01, -7.51559995e-03,\n","       -2.79769987e-01, -1.93140000e-01,  4.62040007e-02,  1.99709997e-01,\n","       -3.02069992e-01,  2.57349998e-01,  6.81070030e-01, -1.94089994e-01,\n","        2.39840001e-01,  2.24930003e-01,  6.52239978e-01, -1.35609999e-01,\n","       -1.73830003e-01, -4.82090004e-02, -1.18600003e-01,  2.15879991e-03,\n","       -1.95250008e-02,  1.19479999e-01,  1.93460003e-01, -4.08199996e-01,\n","       -8.29659998e-02,  1.66260004e-01, -1.06009997e-01,  3.58610004e-01,\n","        1.69220001e-01,  7.25900009e-02, -2.48030007e-01, -1.00240000e-01,\n","       -5.24909973e-01, -1.77450001e-01, -3.66470009e-01,  2.61799991e-01,\n","       -1.20770000e-02,  8.31900015e-02, -2.15279996e-01,  4.10450011e-01,\n","        2.91359991e-01,  3.08690012e-01,  7.88640007e-02,  3.22070003e-01,\n","       -4.10230011e-02, -1.09700002e-01, -9.20410007e-02, -1.23389997e-01,\n","       -1.64159998e-01,  3.53819996e-01, -8.27739984e-02,  3.31710011e-01,\n","       -2.47380003e-01, -4.89280000e-02,  1.57460004e-01,  1.89879999e-01,\n","       -2.66420003e-02,  6.33149967e-02, -1.06729995e-02,  3.40889990e-01,\n","        1.41059995e+00,  1.34169996e-01,  2.81910002e-01, -2.59400010e-01,\n","        5.52669987e-02, -5.24250008e-02, -2.57889986e-01,  1.91270001e-02,\n","       -2.20839996e-02,  3.21130008e-01,  6.88180029e-02,  5.12070000e-01,\n","        1.64780006e-01, -2.01940000e-01,  2.92320013e-01,  9.85750034e-02,\n","        1.31449997e-02, -1.06519997e-01,  1.35100007e-01, -4.53319997e-02,\n","        2.06970006e-01, -4.84250009e-01, -4.47059989e-01,  3.33050010e-03,\n","        2.92639993e-03, -1.09750003e-01, -2.33250007e-01,  2.24419996e-01,\n","       -1.05030000e-01,  1.23389997e-01,  1.09779999e-01,  4.89940010e-02,\n","       -2.51569986e-01,  4.03189987e-01,  3.53179991e-01,  1.86509997e-01,\n","       -2.36220006e-02, -1.27340004e-01,  1.14749998e-01,  2.73589998e-01,\n","       -2.18659997e-01,  1.57939997e-02,  8.17539990e-01, -2.37920005e-02,\n","       -8.54690015e-01, -1.62029997e-01,  1.80759996e-01,  2.80140005e-02,\n","       -1.43399999e-01,  1.31389999e-03, -9.17349979e-02, -8.97039995e-02,\n","        1.11050002e-01, -1.67030007e-01,  6.83770031e-02, -8.73880014e-02,\n","       -3.97889987e-02,  1.41840000e-02,  2.11870000e-01,  2.85789996e-01,\n","       -2.87970006e-01, -5.89959994e-02, -3.24359983e-02, -4.70090006e-03,\n","       -1.70519993e-01, -3.47409993e-02, -1.14890002e-01,  7.50930011e-02,\n","        9.95260030e-02,  4.81830016e-02, -7.37750009e-02, -4.18170005e-01,\n","        4.12680022e-03,  4.44139987e-01, -1.60620004e-01,  1.42940000e-01,\n","       -2.26279998e+00, -2.73470003e-02,  8.13109994e-01,  7.74169981e-01,\n","       -2.56390005e-01, -1.15759999e-01, -1.19819999e-01, -2.13630006e-01,\n","        2.84289997e-02,  2.72610009e-01,  3.10260002e-02,  9.67819989e-02,\n","        6.77690003e-03,  1.40819997e-01, -1.30639998e-02, -2.96860009e-01,\n","       -7.99129978e-02,  1.94999993e-01,  3.15489992e-02,  2.85059988e-01,\n","       -8.74610022e-02,  9.06109996e-03, -2.09889993e-01,  5.39130010e-02])"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"uurgqfamsiI0"},"source":["Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n","\n","Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n","update them during training)."]},{"cell_type":"code","execution_count":69,"metadata":{"id":"_HbnMkZbsiI0","executionInfo":{"status":"ok","timestamp":1701962742688,"user_tz":300,"elapsed":415,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["from tensorflow.keras.layers import Embedding\n","\n","embedding_layer = Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"kd1qf9PQsiI0"},"source":["## Build the model\n","\n","A simple 1D convnet with global max pooling and a classifier at the end."]},{"cell_type":"code","execution_count":76,"metadata":{"id":"r4jS6QUAsiI0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14fd2428-4ad1-43f4-e773-443c73f1c279","executionInfo":{"status":"ok","timestamp":1701963121727,"user_tz":300,"elapsed":480,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_9 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, None, 300)         9000600   \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, None, 256)         384256    \n","                                                                 \n"," max_pooling1d_4 (MaxPoolin  (None, None, 256)         0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, None, 256)         327936    \n","                                                                 \n"," max_pooling1d_5 (MaxPoolin  (None, None, 256)         0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, None, 128)         163968    \n","                                                                 \n"," global_max_pooling1d_2 (Gl  (None, 128)               0         \n"," obalMaxPooling1D)                                               \n","                                                                 \n"," dense_4 (Dense)             (None, 200)               25800     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 200)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 20)                4020      \n","                                                                 \n","=================================================================\n","Total params: 9906580 (37.79 MB)\n","Trainable params: 905980 (3.46 MB)\n","Non-trainable params: 9000600 (34.33 MB)\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import layers\n","\n","int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded_sequences = embedding_layer(int_sequences_input)\n","x = layers.Conv1D(256, 5, activation=\"relu\")(embedded_sequences)\n","x = layers.MaxPooling1D(5)(x)\n","x = layers.Conv1D(256, 5, activation=\"relu\")(x)\n","x = layers.MaxPooling1D(5)(x)\n","x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = layers.Dense(200, activation=\"relu\")(x)\n","x = layers.Dropout(0.25)(x)\n","preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n","model = keras.Model(int_sequences_input, preds)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"YjOsouQMsiI0"},"source":["## Train the model\n","\n","First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n","are right-padded."]},{"cell_type":"code","execution_count":71,"metadata":{"id":"fktZ4cv2siI0","executionInfo":{"status":"ok","timestamp":1701962870886,"user_tz":300,"elapsed":16703,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[],"source":["x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n","x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n","\n","y_train = np.array(train_labels)\n","y_val = np.array(val_labels)"]},{"cell_type":"code","source":["x_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etaZXcLNLkLL","outputId":"e3c4f10c-37a7-4698-b982-a71bb7563eed","executionInfo":{"status":"ok","timestamp":1701725316295,"user_tz":300,"elapsed":160,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4999, 500)"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["x_val[0]"],"metadata":{"id":"VKFVel6ix23h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJNX20KtsiI0"},"source":["We use categorical crossentropy as our loss since we're doing softmax classification.\n","Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."]},{"cell_type":"code","execution_count":77,"metadata":{"id":"BaYYjVqwsiI0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b39c50ab-02a1-4043-f341-2171823cd3d9","executionInfo":{"status":"ok","timestamp":1701963193549,"user_tz":300,"elapsed":58265,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","188/188 [==============================] - 3s 9ms/step - loss: 2.2046 - acc: 0.2680 - val_loss: 1.4368 - val_acc: 0.4889\n","Epoch 2/50\n","188/188 [==============================] - 1s 6ms/step - loss: 1.2055 - acc: 0.5859 - val_loss: 1.0998 - val_acc: 0.6233\n","Epoch 3/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.8691 - acc: 0.7090 - val_loss: 0.9101 - val_acc: 0.7055\n","Epoch 4/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.6258 - acc: 0.7912 - val_loss: 0.9608 - val_acc: 0.7085\n","Epoch 5/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.4489 - acc: 0.8499 - val_loss: 1.0639 - val_acc: 0.7007\n","Epoch 6/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.3261 - acc: 0.8918 - val_loss: 0.8907 - val_acc: 0.7572\n","Epoch 7/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.2523 - acc: 0.9185 - val_loss: 0.9358 - val_acc: 0.7590\n","Epoch 8/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.2016 - acc: 0.9330 - val_loss: 0.9792 - val_acc: 0.7534\n","Epoch 9/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1760 - acc: 0.9432 - val_loss: 1.1268 - val_acc: 0.7566\n","Epoch 10/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1542 - acc: 0.9484 - val_loss: 1.2811 - val_acc: 0.7235\n","Epoch 11/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1340 - acc: 0.9527 - val_loss: 1.4294 - val_acc: 0.7522\n","Epoch 12/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1224 - acc: 0.9570 - val_loss: 1.2100 - val_acc: 0.7706\n","Epoch 13/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1182 - acc: 0.9602 - val_loss: 1.3609 - val_acc: 0.7582\n","Epoch 14/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1101 - acc: 0.9597 - val_loss: 1.2229 - val_acc: 0.7724\n","Epoch 15/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1019 - acc: 0.9616 - val_loss: 1.9188 - val_acc: 0.7061\n","Epoch 16/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1089 - acc: 0.9618 - val_loss: 1.7679 - val_acc: 0.7409\n","Epoch 17/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1079 - acc: 0.9626 - val_loss: 1.5358 - val_acc: 0.7612\n","Epoch 18/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1009 - acc: 0.9651 - val_loss: 1.5939 - val_acc: 0.7622\n","Epoch 19/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0974 - acc: 0.9618 - val_loss: 1.7355 - val_acc: 0.7548\n","Epoch 20/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0935 - acc: 0.9637 - val_loss: 2.2028 - val_acc: 0.7506\n","Epoch 21/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0906 - acc: 0.9650 - val_loss: 1.7650 - val_acc: 0.7734\n","Epoch 22/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0954 - acc: 0.9651 - val_loss: 1.2864 - val_acc: 0.7528\n","Epoch 23/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0793 - acc: 0.9671 - val_loss: 1.9165 - val_acc: 0.7736\n","Epoch 24/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0858 - acc: 0.9667 - val_loss: 1.8993 - val_acc: 0.7714\n","Epoch 25/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0906 - acc: 0.9641 - val_loss: 1.9945 - val_acc: 0.7656\n","Epoch 26/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0993 - acc: 0.9648 - val_loss: 2.1742 - val_acc: 0.7606\n","Epoch 27/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0917 - acc: 0.9675 - val_loss: 2.1973 - val_acc: 0.7624\n","Epoch 28/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0956 - acc: 0.9644 - val_loss: 2.9824 - val_acc: 0.7391\n","Epoch 29/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0915 - acc: 0.9651 - val_loss: 2.6325 - val_acc: 0.7467\n","Epoch 30/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0841 - acc: 0.9675 - val_loss: 3.4710 - val_acc: 0.7245\n","Epoch 31/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0876 - acc: 0.9685 - val_loss: 2.9631 - val_acc: 0.7652\n","Epoch 32/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0858 - acc: 0.9664 - val_loss: 2.5664 - val_acc: 0.7690\n","Epoch 33/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0803 - acc: 0.9667 - val_loss: 2.5246 - val_acc: 0.7544\n","Epoch 34/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0874 - acc: 0.9672 - val_loss: 2.5116 - val_acc: 0.7648\n","Epoch 35/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0799 - acc: 0.9687 - val_loss: 2.4929 - val_acc: 0.7700\n","Epoch 36/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0862 - acc: 0.9676 - val_loss: 2.7753 - val_acc: 0.7600\n","Epoch 37/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0951 - acc: 0.9674 - val_loss: 3.0196 - val_acc: 0.7532\n","Epoch 38/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0970 - acc: 0.9670 - val_loss: 3.7971 - val_acc: 0.7566\n","Epoch 39/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0944 - acc: 0.9675 - val_loss: 3.2196 - val_acc: 0.7576\n","Epoch 40/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0986 - acc: 0.9683 - val_loss: 3.1801 - val_acc: 0.7568\n","Epoch 41/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0866 - acc: 0.9681 - val_loss: 3.1564 - val_acc: 0.7481\n","Epoch 42/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0873 - acc: 0.9692 - val_loss: 3.9153 - val_acc: 0.7375\n","Epoch 43/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0984 - acc: 0.9667 - val_loss: 2.9679 - val_acc: 0.7660\n","Epoch 44/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0863 - acc: 0.9680 - val_loss: 3.2131 - val_acc: 0.7506\n","Epoch 45/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0990 - acc: 0.9677 - val_loss: 4.3482 - val_acc: 0.7612\n","Epoch 46/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.1029 - acc: 0.9675 - val_loss: 3.5231 - val_acc: 0.7241\n","Epoch 47/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0934 - acc: 0.9681 - val_loss: 4.7506 - val_acc: 0.7546\n","Epoch 48/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0970 - acc: 0.9685 - val_loss: 2.7086 - val_acc: 0.7616\n","Epoch 49/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0949 - acc: 0.9689 - val_loss: 2.8789 - val_acc: 0.7664\n","Epoch 50/50\n","188/188 [==============================] - 1s 6ms/step - loss: 0.0931 - acc: 0.9674 - val_loss: 3.5202 - val_acc: 0.7726\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7cc8df709db0>"]},"metadata":{},"execution_count":77}],"source":["model.compile(\n","    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",")\n","model.fit(x_train, y_train, batch_size=80, epochs=50, validation_data=(x_val, y_val))"]},{"cell_type":"markdown","metadata":{"id":"swvds2i9siI0"},"source":["## Export an end-to-end model\n","\n","Now, we may want to export a `Model` object that takes as input a string of arbitrary\n","length, rather than a sequence of indices. It would make the model much more portable,\n","since you wouldn't have to worry about the input preprocessing pipeline.\n","\n","Our `vectorizer` is actually a Keras layer, so it's simple:"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"ZRtppHh8siI0","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"645dac01-f3ca-4fe1-d9d4-142a9a8c2ff4","executionInfo":{"status":"ok","timestamp":1701963237904,"user_tz":300,"elapsed":367,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 170ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["'rec.autos'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":79}],"source":["string_input = keras.Input(shape=(1,), dtype=\"string\")\n","x = vectorizer(string_input)\n","preds = model(x)\n","end_to_end_model = keras.Model(string_input, preds)\n","\n","probabilities = end_to_end_model.predict(\n","    [[\"Yesterday I had a flat tire.\"]]\n",")\n","\n","class_names[np.argmax(probabilities[0])]"]},{"cell_type":"code","source":["class_names"],"metadata":{"id":"AGJ7XJTxE83h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"03d6f2c2-6edf-495f-e047-90d3c04bdf32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['alt.atheism',\n"," 'comp.graphics',\n"," 'comp.os.ms-windows.misc',\n"," 'comp.sys.ibm.pc.hardware',\n"," 'comp.sys.mac.hardware',\n"," 'comp.windows.x',\n"," 'misc.forsale',\n"," 'rec.autos',\n"," 'rec.motorcycles',\n"," 'rec.sport.baseball',\n"," 'rec.sport.hockey',\n"," 'sci.crypt',\n"," 'sci.electronics',\n"," 'sci.med',\n"," 'sci.space',\n"," 'soc.religion.christian',\n"," 'talk.politics.guns',\n"," 'talk.politics.mideast',\n"," 'talk.politics.misc',\n"," 'talk.religion.misc']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":[],"metadata":{"id":"qOOpxURFRoeb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuClass":"premium"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}
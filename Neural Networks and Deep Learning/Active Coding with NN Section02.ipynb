{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854552f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Libraries and more\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "# PyTorch NEEDS a data prep or \"loader\"\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_wine # studied in DATA 201\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1096934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61b8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's revisit the Wine Cultivar dataset\n",
    "x = load_wine().data\n",
    "y = load_wine().target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1830807",
   "metadata": {},
   "source": [
    "## Discussion: What is a Network Topology?\n",
    "\n",
    "- input layer\n",
    "- hidden layer with activations and connectivity\n",
    "- output layer that addresses the machine learning problem to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b33dcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a neural network model\n",
    "# we care about the topology and activations; also we may include regularization such ElasticNet\n",
    "class neuralnet_classfication(nn.Module): # the blueprint of the network\n",
    "    def __init__(self,n_features):\n",
    "        super(neuralnet_classfication, self).__init__()\n",
    "        # here we defined the topology of the neural network\n",
    "        # we want, for example, to have 40 neurons in the first layer\n",
    "        # let's try a funnel shape with dense connections\n",
    "        self.layer1 = nn.Linear(n_features, 40).double()\n",
    "        self.a1 = nn.PReLU(40).double()\n",
    "        self.g1 = nn.GELU().double()\n",
    "        # we designed 20 neurons in the second layer\n",
    "        self.layer2 = nn.Linear(40, 20).double()\n",
    "        self.a2 = nn.PReLU(20).double()\n",
    "        self.layer3 = nn.Linear(20,10).double()\n",
    "        self.a3 = nn.PReLU(10).double()\n",
    "        self.g3 = nn.GELU().double()\n",
    "        # the output layer has only three neurons - the number of neurons is non-neogotiable\n",
    "        self.layer_out = nn.Linear(10, 3).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here is where we actually apply the design to get outputs\n",
    "        x = self.a1(self.layer1(x))\n",
    "        x = self.a2(self.layer2(x))\n",
    "        x = self.a3(self.layer3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "    \n",
    "    def elastic_net_regularization(self, alpha=0.01, l1_ratio=0.5):\n",
    "\n",
    "      l1_reg = 0\n",
    "      l2_reg = 0\n",
    "\n",
    "      # Apply regularization to weights in all linear layers\n",
    "      for name, param in self.named_parameters():\n",
    "          if 'weight' in name:\n",
    "              l1_reg += torch.norm(param, 1).double()  # L1 norm (sum of absolute values)\n",
    "              l2_reg += torch.norm(param, 2).double() ** 2  # L2 norm squared (sum of squares)\n",
    "\n",
    "      # Combine L1 and L2 regularization\n",
    "      reg_loss = alpha *(l1_ratio * l1_reg + 0.5*(1-l1_ratio) * l2_reg)\n",
    "\n",
    "      return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73761ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layer1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1593, -0.0235,  0.0041, -0.1053,  0.1518,  0.2297, -0.0387,  0.0225,\n",
       "            0.1275, -0.0801,  0.1013,  0.1397,  0.1318],\n",
       "          [ 0.2078, -0.0763, -0.0033, -0.0346, -0.1512, -0.1727, -0.0471,  0.1098,\n",
       "            0.1284,  0.2699,  0.0299, -0.2503,  0.1582],\n",
       "          [-0.1821,  0.0766,  0.2101, -0.1789, -0.0852,  0.0995, -0.2194, -0.2164,\n",
       "           -0.0355,  0.0049,  0.0263,  0.2563,  0.0029],\n",
       "          [ 0.1207, -0.1251, -0.2289,  0.0945,  0.0161, -0.0855,  0.0930, -0.2187,\n",
       "            0.0285,  0.0945,  0.2719, -0.1276, -0.1845],\n",
       "          [-0.2210,  0.1306,  0.2043,  0.0736,  0.2051, -0.0037,  0.1827,  0.1189,\n",
       "            0.1398,  0.0732, -0.0522, -0.1289, -0.2663],\n",
       "          [-0.0907, -0.0335,  0.0729, -0.0808,  0.1710, -0.0941,  0.0036,  0.1052,\n",
       "            0.0074,  0.0057, -0.1463,  0.0380,  0.0328],\n",
       "          [-0.0370, -0.0104,  0.0734, -0.1045,  0.0606, -0.2407,  0.1583, -0.1512,\n",
       "            0.1233,  0.1779, -0.0740,  0.2398,  0.0700],\n",
       "          [ 0.1371,  0.0781, -0.0985, -0.2228,  0.1795, -0.0492, -0.1153, -0.2124,\n",
       "           -0.1657,  0.1587, -0.0464, -0.0333, -0.0088],\n",
       "          [-0.1304,  0.0956, -0.1881,  0.1924,  0.1836,  0.0546, -0.0338,  0.1051,\n",
       "            0.0873, -0.0586,  0.2201, -0.1971,  0.0827],\n",
       "          [-0.2103,  0.1329,  0.1824, -0.0054,  0.2445, -0.0906, -0.1564,  0.2262,\n",
       "            0.2584,  0.2598,  0.2353,  0.2437, -0.0915],\n",
       "          [ 0.2070, -0.0217, -0.0671, -0.2247, -0.0360, -0.2620, -0.0160, -0.1215,\n",
       "           -0.1294, -0.2241,  0.0694,  0.1536,  0.1434],\n",
       "          [ 0.1198,  0.0290, -0.0324, -0.0282, -0.2407, -0.2558,  0.1919,  0.2175,\n",
       "            0.1572,  0.0200,  0.1970, -0.0606, -0.2176],\n",
       "          [-0.0130,  0.0786, -0.1320,  0.0040, -0.2187,  0.1157,  0.2285,  0.1593,\n",
       "            0.2317,  0.0120,  0.0308,  0.1212,  0.0467],\n",
       "          [-0.0635, -0.2591, -0.0040,  0.0640,  0.2471, -0.1185,  0.0429, -0.1733,\n",
       "           -0.1131, -0.2019,  0.1724, -0.0820, -0.0712],\n",
       "          [ 0.2500, -0.0414,  0.1943,  0.0163, -0.1775,  0.2629,  0.1359, -0.0846,\n",
       "           -0.1447,  0.1307,  0.0794, -0.0430,  0.1230],\n",
       "          [ 0.2015,  0.1949, -0.0559, -0.2194,  0.1235, -0.0922,  0.0507, -0.0959,\n",
       "            0.1797, -0.0487,  0.1797,  0.2483, -0.0319],\n",
       "          [-0.0259, -0.1780, -0.2311, -0.2737,  0.1977,  0.1680, -0.1559,  0.2424,\n",
       "           -0.1921, -0.1541, -0.2646,  0.1344, -0.0901],\n",
       "          [-0.0668,  0.1772,  0.2255, -0.1764, -0.1104, -0.0058, -0.0862, -0.1616,\n",
       "            0.0291, -0.0824, -0.1672, -0.1733, -0.1364],\n",
       "          [ 0.1739, -0.0377, -0.2035, -0.1456,  0.2173,  0.0490,  0.1442,  0.2723,\n",
       "           -0.1157,  0.1971,  0.0566, -0.1671,  0.0666],\n",
       "          [-0.1932, -0.2246,  0.0686, -0.1615,  0.2297, -0.0051, -0.0269,  0.0173,\n",
       "           -0.2172,  0.1253, -0.1244, -0.1079, -0.2556],\n",
       "          [-0.1927,  0.1026, -0.0309,  0.0848, -0.0019,  0.0230, -0.1985, -0.2504,\n",
       "           -0.1690, -0.1987,  0.0707, -0.2219, -0.1429],\n",
       "          [-0.0255, -0.1374,  0.1043, -0.1758, -0.2765, -0.2732, -0.1508, -0.0745,\n",
       "            0.2463,  0.2209, -0.0872,  0.0763,  0.0457],\n",
       "          [-0.1953, -0.0577, -0.1906, -0.1104,  0.2501, -0.1424, -0.0661, -0.1383,\n",
       "            0.1078, -0.2710, -0.1526, -0.0538,  0.0644],\n",
       "          [ 0.2321, -0.2210, -0.2651,  0.1531,  0.0459, -0.2406,  0.1839, -0.1897,\n",
       "            0.2365,  0.2024,  0.0881,  0.1826, -0.1311],\n",
       "          [-0.2400, -0.0479, -0.0621, -0.2286,  0.1320, -0.1482,  0.1470, -0.1304,\n",
       "           -0.0297, -0.2143, -0.1326, -0.0494, -0.1230],\n",
       "          [ 0.0938, -0.0560,  0.1128,  0.1778,  0.0391, -0.2709, -0.0882, -0.2122,\n",
       "            0.0277,  0.1539,  0.1678,  0.2111,  0.0764],\n",
       "          [-0.0887,  0.2064,  0.1269,  0.1749,  0.0951, -0.1481,  0.1328, -0.0652,\n",
       "            0.0216, -0.0971, -0.0298,  0.0554,  0.1199],\n",
       "          [ 0.2384,  0.1233, -0.1076,  0.1352, -0.2276,  0.1012,  0.1070,  0.1330,\n",
       "            0.2262,  0.1646,  0.1264, -0.0456, -0.2419],\n",
       "          [ 0.1629, -0.0324,  0.1863,  0.0869, -0.1246,  0.2325, -0.1720,  0.2676,\n",
       "           -0.2771, -0.1056, -0.1971,  0.2762,  0.1416],\n",
       "          [ 0.2158, -0.0178,  0.0556, -0.1795,  0.0489,  0.2326,  0.1936,  0.0897,\n",
       "            0.2583,  0.0873, -0.1083,  0.0661,  0.2178],\n",
       "          [-0.0659,  0.2104, -0.0980,  0.1613, -0.2276, -0.2324, -0.0398,  0.1055,\n",
       "            0.1456,  0.0649,  0.2112,  0.0163,  0.2056],\n",
       "          [ 0.0267, -0.1827,  0.1072, -0.2667, -0.1852, -0.2192, -0.1164, -0.0195,\n",
       "            0.0962, -0.0202,  0.2557, -0.0990,  0.1292],\n",
       "          [ 0.2487,  0.1647, -0.1741, -0.0231, -0.2362, -0.1412,  0.0788, -0.0259,\n",
       "           -0.0251,  0.0758,  0.1392,  0.2301, -0.0132],\n",
       "          [-0.0466, -0.0804, -0.2105,  0.1960, -0.0464, -0.1289, -0.2408, -0.1857,\n",
       "           -0.2731, -0.2304,  0.2762,  0.2044,  0.0608],\n",
       "          [-0.1569,  0.1434,  0.0158, -0.0735,  0.0040,  0.0665, -0.0413,  0.0063,\n",
       "           -0.2700,  0.2001, -0.0323,  0.0217,  0.0313],\n",
       "          [-0.0238,  0.0924, -0.2669, -0.0688,  0.0915,  0.2343, -0.1283, -0.1835,\n",
       "           -0.0026,  0.0788, -0.1904,  0.0407,  0.0782],\n",
       "          [-0.2730, -0.2223, -0.2642,  0.0579, -0.1766, -0.1685,  0.2517, -0.0235,\n",
       "            0.0709, -0.2667,  0.1447, -0.1444,  0.2263],\n",
       "          [-0.2518, -0.2657, -0.0513,  0.2356, -0.0257,  0.1024,  0.2490, -0.0067,\n",
       "            0.1477, -0.2401, -0.0891,  0.1622,  0.2111],\n",
       "          [-0.0797, -0.0808, -0.0872, -0.2501, -0.0226,  0.0429,  0.1445,  0.1823,\n",
       "            0.0961,  0.1551, -0.2515, -0.0915, -0.1799],\n",
       "          [-0.0539, -0.2102, -0.1302, -0.0271, -0.2314,  0.1391,  0.1772,  0.2224,\n",
       "            0.0853,  0.0187,  0.1485, -0.0078,  0.1424]], device='cuda:0',\n",
       "         dtype=torch.float64, requires_grad=True)),\n",
       " ('layer1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0693, -0.0275, -0.1421,  0.0632,  0.2404,  0.2321, -0.1419, -0.2424,\n",
       "           0.0554,  0.0875,  0.2743,  0.0565,  0.1091, -0.0975, -0.1402,  0.2740,\n",
       "          -0.1726,  0.2472,  0.2124,  0.2525, -0.1330,  0.0165,  0.0345, -0.2369,\n",
       "           0.1663,  0.2155, -0.1505,  0.1416,  0.1579, -0.2284, -0.0551, -0.0965,\n",
       "           0.1224,  0.0704,  0.0782, -0.1641, -0.2340,  0.1960,  0.1331,  0.2727],\n",
       "         device='cuda:0', dtype=torch.float64, requires_grad=True)),\n",
       " ('a1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0',\n",
       "         dtype=torch.float64, requires_grad=True)),\n",
       " ('layer2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0382,  0.1225,  0.1115, -0.0878, -0.0566,  0.1419,  0.0635, -0.0641,\n",
       "           -0.1027,  0.0893, -0.0098,  0.0416, -0.0760,  0.0763, -0.1355,  0.0546,\n",
       "           -0.1463, -0.1515, -0.1455, -0.0401, -0.0932, -0.0011, -0.0980,  0.1547,\n",
       "            0.0100,  0.0551, -0.0616, -0.0526, -0.0846, -0.1140, -0.0936,  0.1489,\n",
       "           -0.0419, -0.1029, -0.1298,  0.0274,  0.0436, -0.0174, -0.0191, -0.1338],\n",
       "          [-0.0206,  0.1077, -0.1021, -0.0889,  0.0303, -0.0658, -0.0861,  0.0447,\n",
       "            0.1531,  0.0070,  0.0272, -0.0006, -0.1540,  0.0916,  0.0586, -0.1342,\n",
       "            0.0927,  0.1042,  0.0360, -0.0974,  0.1155, -0.1498, -0.0011, -0.0853,\n",
       "           -0.0199, -0.1424, -0.0505,  0.0373,  0.1164, -0.0932, -0.1062, -0.0994,\n",
       "            0.0452,  0.0049, -0.0423,  0.0545, -0.0840,  0.0294, -0.0920, -0.1553],\n",
       "          [ 0.0080,  0.1318, -0.0522,  0.0601,  0.0375, -0.0492,  0.1556, -0.0578,\n",
       "           -0.0527,  0.1120, -0.0867, -0.0088, -0.0249, -0.1440, -0.0358,  0.0813,\n",
       "           -0.0629,  0.0279,  0.1062,  0.0755, -0.1101,  0.0792,  0.0403, -0.0962,\n",
       "            0.0454,  0.1155,  0.0389, -0.1016, -0.1331,  0.1378, -0.1328,  0.0679,\n",
       "            0.0213, -0.0958, -0.0428, -0.0714, -0.0373,  0.0224,  0.0098, -0.0113],\n",
       "          [ 0.1534, -0.0012,  0.0604, -0.0490, -0.1451, -0.0387,  0.0484, -0.0468,\n",
       "            0.1469, -0.1572,  0.0522, -0.1280,  0.1532, -0.0766, -0.1411, -0.0606,\n",
       "           -0.0924, -0.1030,  0.0842, -0.0754, -0.0914, -0.0844,  0.1231,  0.0935,\n",
       "            0.1266, -0.0737, -0.0830, -0.0331,  0.0261, -0.0755,  0.0395,  0.0232,\n",
       "            0.0018,  0.0696,  0.1039, -0.0021, -0.1335,  0.1068,  0.0122, -0.0404],\n",
       "          [-0.1519, -0.0446, -0.1106,  0.0502, -0.0317,  0.1111,  0.1576,  0.1581,\n",
       "           -0.1188, -0.0581,  0.1422,  0.1324,  0.0414,  0.0051, -0.1345,  0.1040,\n",
       "            0.0493, -0.0222, -0.0569, -0.0233, -0.0909,  0.0115, -0.0986, -0.1078,\n",
       "            0.1223,  0.0674, -0.0220, -0.0665, -0.1397, -0.0444, -0.1373, -0.1007,\n",
       "            0.0311, -0.0967, -0.0381, -0.0053,  0.1277, -0.1136,  0.0768,  0.0399],\n",
       "          [ 0.0974,  0.1482, -0.1528, -0.0555,  0.1486, -0.1356,  0.0512, -0.0782,\n",
       "            0.0521,  0.0019,  0.1044, -0.1392, -0.0191, -0.0356,  0.0629,  0.0617,\n",
       "           -0.0603, -0.1201,  0.0470, -0.0479, -0.0374,  0.0117,  0.0280,  0.0153,\n",
       "           -0.0072, -0.0062, -0.0518,  0.0161, -0.0770, -0.0035, -0.1545,  0.1471,\n",
       "           -0.1350, -0.0715,  0.0333, -0.1333, -0.1214,  0.0183,  0.0690, -0.0040],\n",
       "          [ 0.0426, -0.0514,  0.1314, -0.0798,  0.1003, -0.1048, -0.1155,  0.0710,\n",
       "           -0.0593, -0.1041,  0.0025, -0.0123,  0.1213,  0.0938,  0.1503,  0.0574,\n",
       "           -0.1013, -0.0765,  0.0475,  0.0378, -0.0723,  0.0097,  0.0034, -0.1113,\n",
       "            0.1041, -0.0966, -0.0476,  0.0837,  0.0315,  0.0796, -0.0973,  0.0372,\n",
       "           -0.1323, -0.1333,  0.0043,  0.0848,  0.1064, -0.0936, -0.1084,  0.0291],\n",
       "          [ 0.1254, -0.0980,  0.0015,  0.0783, -0.0060,  0.1391,  0.1247, -0.0058,\n",
       "            0.1153, -0.1009,  0.1009,  0.0165, -0.1314,  0.0627,  0.0144, -0.1050,\n",
       "           -0.0594,  0.0854,  0.1516, -0.0392,  0.1372, -0.0347,  0.0241, -0.0916,\n",
       "            0.0061, -0.0716, -0.0997, -0.1271, -0.1089,  0.1093,  0.1006, -0.1028,\n",
       "           -0.0355,  0.0051,  0.0199, -0.0096,  0.0842,  0.0770,  0.0184,  0.0917],\n",
       "          [ 0.0608,  0.1400, -0.0445,  0.0551,  0.0295, -0.0560,  0.1412,  0.0183,\n",
       "            0.1336,  0.1011, -0.0643, -0.0514, -0.1462,  0.1510,  0.0277, -0.0082,\n",
       "            0.0079, -0.0871, -0.1025,  0.1059,  0.0212,  0.0482,  0.1069,  0.0557,\n",
       "            0.1343,  0.0056, -0.0281,  0.0899,  0.1558, -0.0556, -0.1132,  0.0243,\n",
       "           -0.1002,  0.0948, -0.0828, -0.1449,  0.1144, -0.1414,  0.0357, -0.0277],\n",
       "          [ 0.1006,  0.1518,  0.0708, -0.1062,  0.0593, -0.0128,  0.1569,  0.0466,\n",
       "            0.0144,  0.0834,  0.0671, -0.0099,  0.0723, -0.0196, -0.1044, -0.0978,\n",
       "           -0.0965,  0.0106, -0.0175,  0.0914, -0.0578, -0.0571,  0.0129,  0.0411,\n",
       "            0.0770,  0.0246, -0.0805,  0.0798, -0.0574, -0.0862, -0.0269, -0.0551,\n",
       "           -0.1295, -0.0074,  0.0567, -0.1209, -0.1360,  0.0514, -0.0859, -0.1270],\n",
       "          [-0.0590, -0.0396,  0.1343, -0.0277, -0.0658,  0.1420, -0.0189, -0.0918,\n",
       "            0.0268,  0.0415,  0.0648, -0.0928,  0.1403, -0.0135, -0.0155,  0.0496,\n",
       "            0.0723, -0.1382,  0.0567,  0.1291, -0.1058, -0.1136,  0.0846,  0.0154,\n",
       "           -0.1354, -0.0290, -0.1474, -0.0756,  0.0588, -0.1345, -0.0500, -0.1521,\n",
       "           -0.0178, -0.0164,  0.0065,  0.1377,  0.1224,  0.0920, -0.0962, -0.0036],\n",
       "          [ 0.0458, -0.1278,  0.0932, -0.0594,  0.0636, -0.1234, -0.0250, -0.0520,\n",
       "           -0.1439, -0.0735, -0.1435,  0.1259, -0.0787, -0.0835, -0.0544, -0.1410,\n",
       "            0.0059, -0.0912, -0.0665, -0.0907,  0.0441, -0.0998,  0.1390,  0.0845,\n",
       "           -0.0467,  0.1472,  0.1422, -0.0219, -0.1524, -0.0033, -0.1095,  0.0495,\n",
       "           -0.0051,  0.0967,  0.0815, -0.0631, -0.1319, -0.0749,  0.0942,  0.1559],\n",
       "          [-0.1318,  0.1467, -0.1086, -0.1447,  0.1107,  0.0165,  0.1014, -0.0280,\n",
       "           -0.0931,  0.1084,  0.0089, -0.0614,  0.0080, -0.1289,  0.0181, -0.0243,\n",
       "           -0.0475,  0.0600,  0.0249, -0.1131,  0.1151, -0.1543,  0.0796,  0.0291,\n",
       "            0.1462, -0.0559,  0.0162,  0.0733, -0.1580, -0.0024, -0.1554,  0.1426,\n",
       "           -0.0429, -0.0289, -0.0450, -0.1296,  0.0951, -0.0737, -0.0106, -0.0531],\n",
       "          [ 0.1192, -0.0751,  0.0793, -0.0112, -0.0872, -0.1499,  0.0772, -0.1531,\n",
       "           -0.1411,  0.0884,  0.1445, -0.0255,  0.0931,  0.0305,  0.0351,  0.0390,\n",
       "           -0.0057,  0.0032,  0.0930, -0.0611,  0.0623,  0.1304, -0.1489, -0.0510,\n",
       "            0.0663, -0.0784,  0.0222, -0.0860, -0.0349, -0.0829,  0.0984, -0.1282,\n",
       "           -0.0953,  0.0945,  0.0968,  0.1233,  0.0791,  0.0386, -0.1409,  0.1280],\n",
       "          [ 0.0233, -0.0911,  0.0704,  0.1561, -0.1530,  0.0187, -0.1007,  0.0349,\n",
       "            0.0297, -0.1358, -0.1578, -0.0273,  0.1145, -0.1521, -0.0943, -0.1218,\n",
       "           -0.0685,  0.1169,  0.0330,  0.0720, -0.0370,  0.1136,  0.0953, -0.1578,\n",
       "            0.0069, -0.0933,  0.1363,  0.0030,  0.1009, -0.1575,  0.0231, -0.0467,\n",
       "            0.1076, -0.0528,  0.0025,  0.0112, -0.1260,  0.0840,  0.1377,  0.0813],\n",
       "          [-0.0013, -0.0415, -0.0082, -0.1271, -0.0276,  0.0009,  0.0551, -0.0681,\n",
       "            0.0085,  0.0441, -0.1572,  0.0858, -0.1051,  0.1077, -0.0755, -0.0203,\n",
       "           -0.0425, -0.0306,  0.1578, -0.1245, -0.1376,  0.1246,  0.0155, -0.0420,\n",
       "            0.1016, -0.0235, -0.1081,  0.1161, -0.0957, -0.0118, -0.0399,  0.1004,\n",
       "            0.0851, -0.0377, -0.0914,  0.0986,  0.0570,  0.0273,  0.0337,  0.0302],\n",
       "          [ 0.0561, -0.1292, -0.0339,  0.1524, -0.0398,  0.1186, -0.1335, -0.0858,\n",
       "           -0.0179, -0.1229, -0.0581, -0.0979,  0.0056,  0.0847, -0.0391, -0.0027,\n",
       "            0.0855,  0.0280, -0.0177,  0.0249,  0.1184, -0.0979, -0.0778, -0.0026,\n",
       "           -0.1275, -0.1484, -0.0902,  0.0156, -0.1175, -0.0926, -0.0599,  0.1541,\n",
       "           -0.0976, -0.0096, -0.0913,  0.0921,  0.1049, -0.0646,  0.0742,  0.1410],\n",
       "          [-0.0813, -0.0236, -0.0051,  0.1570, -0.0102,  0.0867, -0.1492, -0.1333,\n",
       "           -0.0977,  0.0008,  0.0378, -0.0767,  0.0134,  0.1357, -0.0211, -0.0751,\n",
       "           -0.1108,  0.1545,  0.1570, -0.1545,  0.0662, -0.1207,  0.0500, -0.0397,\n",
       "            0.0742, -0.0315,  0.0299,  0.0820,  0.1233, -0.0731, -0.1432, -0.1469,\n",
       "           -0.0616, -0.0921,  0.0299, -0.0329,  0.0107,  0.1165, -0.0214,  0.0853],\n",
       "          [ 0.0397, -0.0828,  0.0965, -0.1201, -0.1509,  0.1179,  0.1313,  0.1076,\n",
       "           -0.0261, -0.1074, -0.0945, -0.0889,  0.1181,  0.0556, -0.0557, -0.0044,\n",
       "           -0.0819, -0.1118, -0.0505,  0.1123, -0.1156,  0.0290,  0.0779,  0.1573,\n",
       "            0.0795,  0.0601, -0.0853,  0.1434, -0.0064,  0.0982,  0.1299,  0.0898,\n",
       "           -0.0298,  0.0277,  0.1183,  0.1201,  0.0474, -0.0934,  0.1179, -0.0120],\n",
       "          [-0.0970,  0.0896, -0.1269, -0.0599, -0.0110,  0.0322, -0.0646, -0.1274,\n",
       "            0.0666, -0.0415, -0.1015, -0.0655, -0.1424,  0.1428,  0.0901, -0.1323,\n",
       "            0.0223, -0.0537, -0.0055, -0.0395,  0.0557, -0.0527, -0.0756,  0.0796,\n",
       "            0.1376, -0.0991, -0.0635,  0.1487,  0.1042,  0.1003,  0.1499,  0.1357,\n",
       "           -0.0672, -0.1325, -0.0811, -0.1473, -0.0185,  0.0533, -0.0196,  0.1282]],\n",
       "         device='cuda:0', dtype=torch.float64, requires_grad=True)),\n",
       " ('layer2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0672,  0.0923, -0.1522, -0.0141,  0.1021, -0.1497,  0.0608,  0.1523,\n",
       "           0.1141, -0.0667,  0.1193,  0.0204,  0.0092, -0.1209,  0.1411,  0.0423,\n",
       "          -0.0733, -0.1044,  0.0231, -0.1068], device='cuda:0',\n",
       "         dtype=torch.float64, requires_grad=True)),\n",
       " ('a2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 0.2500], device='cuda:0', dtype=torch.float64,\n",
       "         requires_grad=True)),\n",
       " ('layer3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1020,  0.0039,  0.0789,  0.0933, -0.1580,  0.0265,  0.1807, -0.2142,\n",
       "            0.0504, -0.1974,  0.1990, -0.0281,  0.1014, -0.1527, -0.0170, -0.1584,\n",
       "            0.1057,  0.0982, -0.1307, -0.0851],\n",
       "          [ 0.0544, -0.0265,  0.0920,  0.2003, -0.1262, -0.1690,  0.1987, -0.0979,\n",
       "           -0.1535,  0.0263, -0.0053, -0.0986,  0.2229, -0.1629, -0.1460, -0.0617,\n",
       "            0.1395, -0.1826,  0.1437,  0.1834],\n",
       "          [-0.1492,  0.0898, -0.0732, -0.1457, -0.0578, -0.1376, -0.0947,  0.1102,\n",
       "           -0.1291,  0.1053, -0.0987,  0.1629,  0.0126,  0.1764, -0.1513, -0.0729,\n",
       "           -0.0745, -0.1883,  0.1780, -0.0968],\n",
       "          [ 0.0322,  0.1706, -0.0943, -0.0367, -0.1230, -0.0362,  0.0535, -0.1298,\n",
       "            0.1800,  0.1219,  0.0903,  0.1543, -0.0538, -0.0738, -0.2216, -0.0187,\n",
       "           -0.1833, -0.0251,  0.1577,  0.2047],\n",
       "          [ 0.0953,  0.1633, -0.2171, -0.0411,  0.0590, -0.0375, -0.1632, -0.0532,\n",
       "           -0.1528, -0.1009, -0.1002,  0.0740, -0.0983,  0.0252,  0.1278, -0.1492,\n",
       "           -0.0968,  0.1066,  0.2184, -0.0445],\n",
       "          [ 0.1397, -0.2140,  0.1884,  0.0785, -0.1883, -0.0134, -0.1560,  0.1991,\n",
       "            0.0789,  0.1429, -0.1093,  0.0536,  0.1944,  0.1065, -0.1783, -0.1470,\n",
       "            0.1818, -0.0227,  0.1880, -0.0691],\n",
       "          [ 0.0585, -0.1619, -0.2171, -0.0700,  0.0868, -0.0585,  0.2105, -0.0320,\n",
       "            0.1830,  0.0477,  0.1002, -0.0397,  0.1898, -0.1872, -0.2080,  0.1386,\n",
       "            0.1598, -0.1993,  0.1625,  0.1871],\n",
       "          [ 0.0336, -0.0332, -0.1563, -0.1089,  0.0917, -0.1292,  0.0849,  0.1233,\n",
       "           -0.0991, -0.1438, -0.1669, -0.1885, -0.0806, -0.1228,  0.1682,  0.0091,\n",
       "           -0.0264,  0.2143,  0.0369,  0.1393],\n",
       "          [-0.1319, -0.0187, -0.1727, -0.0029, -0.2169, -0.1481,  0.1299, -0.1034,\n",
       "            0.0247,  0.0667, -0.1204, -0.0964,  0.1236, -0.1660, -0.1642, -0.0751,\n",
       "           -0.0683,  0.0507,  0.0851,  0.1267],\n",
       "          [ 0.1128,  0.1465, -0.2083,  0.0337,  0.0694, -0.0862, -0.0226,  0.0671,\n",
       "            0.0585,  0.0227,  0.0917, -0.1666, -0.0405, -0.0609, -0.1160,  0.1779,\n",
       "            0.0684, -0.0610,  0.2220, -0.1061]], device='cuda:0',\n",
       "         dtype=torch.float64, requires_grad=True)),\n",
       " ('layer3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0414,  0.0541,  0.1122,  0.1581,  0.1382,  0.0723,  0.2051, -0.0502,\n",
       "           0.0442,  0.0169], device='cuda:0', dtype=torch.float64,\n",
       "         requires_grad=True)),\n",
       " ('a3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500], device='cuda:0', dtype=torch.float64, requires_grad=True)),\n",
       " ('layer_out.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1782,  0.2511, -0.1659,  0.1557,  0.2749, -0.0461,  0.0235,  0.0588,\n",
       "            0.1083, -0.2474],\n",
       "          [ 0.2782,  0.1023,  0.0148, -0.0232, -0.2794, -0.2868,  0.1047,  0.0510,\n",
       "            0.2973,  0.1243],\n",
       "          [-0.1076,  0.0478,  0.2381,  0.0984, -0.2628,  0.1800, -0.2204, -0.0854,\n",
       "            0.0349, -0.0710]], device='cuda:0', dtype=torch.float64,\n",
       "         requires_grad=True)),\n",
       " ('layer_out.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.1194, 0.0934, 0.1805], device='cuda:0', dtype=torch.float64,\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c66289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the data\n",
    "# Standardize the features\n",
    "scale = StandardScaler()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=301)\n",
    "\n",
    "# we need to put the data into pytorch tensors\n",
    "# Convert the data to PyTorch tensors -> are the anolog of Numpy Arrays\n",
    "X_train_tensor = torch.tensor(scale.fit_transform(X_train), dtype=torch.float64).to(device) # it means double precision\n",
    "X_test_tensor = torch.tensor(scale.transform(X_test), dtype=torch.float64).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# i want to prepare an iterable object with mini-batches\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# this is for automating mini-stochastic batches\n",
    "# so the method DataLoader is creating a iterable objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b2e9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = neuralnet_classfication(X_train.shape[1]).to(device)\n",
    "# the loss function (or criterion) is based on the problem you want to solve\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# we also have to pick a choice of a gradient-based method to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e2e3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 30 # an epoch means a complete passage through all batches\n",
    "for epoch in range(num_epochs):\n",
    "    # a peculiar aspect of Pytorch -> you set the model in a \"training\" state\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "        optimizer.zero_grad() # resets the partial derivatives\n",
    "        # do a forward propagation\n",
    "        outputs = model(X_batch) # forward propagates\n",
    "        # use the criterion to compute the loss of the batch\n",
    "        main_loss = criterion(outputs, y_batch) # computes the loss\n",
    "        reg_loss = model.elastic_net_regularization(alpha=0.1,l1_ratio=0.5)\n",
    "        total_loss = main_loss + reg_loss\n",
    "        # here we backpropagate\n",
    "        total_loss.backward() # approximates the gradient\n",
    "        optimizer.step() # updates the model parameters\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b67510d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.3968\n"
     ]
    }
   ],
   "source": [
    "# we check the accuracy\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # here we lock against contamination the model parameters\n",
    "with torch.no_grad():\n",
    "    y_pred_list = [] # these are necessary tracker variables to extract the information\n",
    "    y_true_list = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # here the outputs are the forward propagation of the model\n",
    "        # at the learned weights\n",
    "        outputs = model(X_batch)\n",
    "        # here we apply the softmax\n",
    "        _, y_pred = torch.max(outputs, 1) # this is picking the highest probability for each class\n",
    "        y_pred_list.append(y_pred)\n",
    "        y_true_list.append(y_batch)\n",
    "\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    accuracy = accuracy_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc2236a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression, test a similar design with the concrete data and also experiment with Gelu activations and ELastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed351c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_p13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

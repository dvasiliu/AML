{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9b2e91-ca91-4f6b-b24c-ee88bb6cc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d4994e-3fa5-4ecb-86d4-d0ce25707c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMLE_Improved:\n",
    "    def __init__(self, regularization=1.0, max_iter=1000):\n",
    "        self.C = regularization  # Inverse of regularization strength\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def log_likelihood_with_regularization(self, params, X, y):\n",
    "\n",
    "        # Negative log-likelihood with L2 regularization.\n",
    "        # We want to minimize: -log_likelihood + (1/(2*C)) * ||weights||^2\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        weights = params[:n_features]\n",
    "        bias = params[n_features]\n",
    "        \n",
    "        # Predictions\n",
    "        linear_pred = np.dot(X, weights) + bias\n",
    "        predictions = self.sigmoid(linear_pred)\n",
    "        predictions = np.clip(predictions, 1e-10, 1 - 1e-10)\n",
    "        \n",
    "        # Negative log-likelihood\n",
    "        log_lik = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "        \n",
    "        # L2 regularization term (only on weights, not bias)\n",
    "        regularization_term = (1 / (2 * self.C)) * np.sum(weights ** 2)\n",
    "        \n",
    "        return -log_lik + regularization_term\n",
    "    \n",
    "    def gradient(self, params, X, y):\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        weights = params[:n_features]\n",
    "        bias = params[n_features]\n",
    "        \n",
    "        linear_pred = np.dot(X, weights) + bias\n",
    "        predictions = self.sigmoid(linear_pred)\n",
    "        \n",
    "        # Gradient of negative log-likelihood\n",
    "        error = predictions - y\n",
    "        grad_weights = np.dot(X.T, error) + (1 / self.C) * weights\n",
    "        grad_bias = np.sum(error)\n",
    "        \n",
    "        return np.concatenate([grad_weights, [grad_bias]])\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        n_features = X_scaled.shape[1]\n",
    "        initial_params = np.zeros(n_features + 1)\n",
    "        \n",
    "        # Minimize with gradient information\n",
    "        result = minimize(\n",
    "            fun=self.log_likelihood_with_regularization,\n",
    "            x0=initial_params,\n",
    "            args=(X_scaled, y),\n",
    "            method='L-BFGS-B',  # Better for this problem\n",
    "            jac=self.gradient,  # Provide gradient\n",
    "            options={'maxiter': self.max_iter}\n",
    "        )\n",
    "        \n",
    "        self.weights = result.x[:n_features]\n",
    "        self.bias = result.x[n_features]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Optimization successful: {result.success}\")\n",
    "            print(f\"Final negative log-likelihood: {result.fun:.4f}\")\n",
    "            print(f\"Number of iterations: {result.nit}\")\n",
    "        \n",
    " \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        linear_pred = np.dot(X_scaled, self.weights) + self.bias\n",
    "        return self.sigmoid(linear_pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Example usage:\n",
    "# model = LogisticRegressionMLE_Improved(regularization=1.0)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8a4b94-8fa6-44e1-bd82-b083939a7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive approach\n",
    "# let's make a blueprint (a class) that can instantiate objects\n",
    "class LR:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def sig(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def gradient(self,x,y,w,b):\n",
    "        errors = y - self.sig(x@w+b)\n",
    "        return -1/len(x)*(errors)@x, -1/len(x)*sum(errors)\n",
    "    def fit(self,x,y,lr=0.01,maxiter=1000,intercept=True,tol=1e-5):\n",
    "        # the main goal of this is to update the weights with gradient descent\n",
    "        # first initialize the weights and the bias term\n",
    "        self.w = np.random.normal(size=x.shape[1])\n",
    "        if intercept:\n",
    "            self.b = np.random.normal()\n",
    "        else:\n",
    "            self.b = 0\n",
    "        for _ in range(maxiter):\n",
    "            gw, gb = self.gradient(x,y,self.w,self.b)\n",
    "            self.wnew = self.w - lr*gw\n",
    "            self.bnew = self.b - lr*gb\n",
    "            if np.linalg.norm(self.wnew-self.w)<tol:\n",
    "                break\n",
    "            self.w = self.wnew\n",
    "            self.b = self.bnew\n",
    "            \n",
    "    def predict_proba(self,x):\n",
    "        return self.sig(x@self.w+self.b)\n",
    "\n",
    "    def predict_classes(self,x,thresh=0.5):\n",
    "        return (self.sig(x@self.w+self.b)>thresh) + 0 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb26363-c0f6-492d-9f14-80a1177941a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup \n",
    "w = np.random.uniform(size=x.shape[1])\n",
    "b  = np.random.uniform()\n",
    "lr = 0.0005\n",
    "maxiter = 10000\n",
    "tol = 1e-5\n",
    "mse = []\n",
    "u = 0\n",
    "beta1 = 0.9\n",
    "done = False\n",
    "batch_size = 32\n",
    "sw = 0\n",
    "sb = 0\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "818185f6-9492-4f30-bae1-92dfe242c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSPROP approach\n",
    "class LR_rmsprop:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def loss(self,x,y):\n",
    "        predictions = self.sig(x@self.w+self.b)\n",
    "        log_lik = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "        return -log_lik\n",
    "\n",
    "    def sig(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "        \n",
    "    def gradient(self,x,y,w,b):\n",
    "        errors = y - self.sig(x@w+b)\n",
    "        return -1/len(x)*(errors)@x, -1/len(x)*sum(errors)\n",
    "        \n",
    "    def fit(self,x,y,lr=0.01,maxiter=1000,intercept=True,tol=1e-5,sw=0,sb=0,eps=1e-6,batch_size=32,done=False,beta1=0.9,u=0):\n",
    "        # the main goal of this is to update the weights with gradient descent\n",
    "        # first initialize the weights and the bias term\n",
    "        self.w = np.random.normal(size=x.shape[1])\n",
    "        if intercept:\n",
    "            self.b = np.random.normal()\n",
    "        else:\n",
    "            self.b = 0\n",
    "        lss= []\n",
    "        for i in range(maxiter):\n",
    "            minibatches = np.array_split(np.random.permutation(range(len(x))),len(x)//batch_size)\n",
    "            for batch in minibatches:\n",
    "                gw, gb = self.gradient(x,y,self.w,self.b)\n",
    "        \n",
    "                # here we create adaptive learning rates\n",
    "                sw = beta1*sw + (1-beta1)*sum(gw**2)\n",
    "                sb = beta1*sb + (1-beta1)*gb**2\n",
    "                \n",
    "                self.wnew = self.w - lr/np.sqrt(sw+eps)*gw\n",
    "                self.bnew = self.b - lr/np.sqrt(sb+eps)*gb\n",
    "                u += 1\n",
    "                lss.append(self.loss(x,y))\n",
    "                if np.linalg.norm(self.wnew-self.w)<tol:\n",
    "                    print('The Algorithm has Converged!')\n",
    "                    done = True\n",
    "                    break\n",
    "                if (u+1)%100 ==0:\n",
    "                    print(f'After {u+1} updates the Loss is: {self.loss(x,y)}')\n",
    "                self.w = self.wnew\n",
    "                self.b = self.bnew\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    def predict_proba(self,x):\n",
    "        return self.sig(x@self.w+self.b)\n",
    "\n",
    "    def predict_classes(self,x,thresh=0.5):\n",
    "        return (self.sig(x@self.w+self.b)>thresh) + 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31ad0a81-de56-44a1-aad4-5784b21a633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/dvasiliu/AML/blob/main/Data%20Sets/example_data_classification.csv?raw=true', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2ed5844-0298-4ba0-8452-febdaf88b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,[0,1]].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63275154-966c-4a1a-8c90-7c874367a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 updates the Loss is: 5664.759552898149\n",
      "After 200 updates the Loss is: 439.42535223897215\n",
      "After 300 updates the Loss is: 72.3688597835951\n",
      "After 400 updates the Loss is: 70.41067712375975\n",
      "After 500 updates the Loss is: 66.24164877728774\n",
      "After 600 updates the Loss is: 62.48898565969772\n",
      "After 700 updates the Loss is: 59.11642023793128\n",
      "After 800 updates the Loss is: 56.08773607365078\n",
      "After 900 updates the Loss is: 53.3679038582983\n",
      "After 1000 updates the Loss is: 50.9239220204625\n",
      "After 1100 updates the Loss is: 48.72532948881079\n",
      "After 1200 updates the Loss is: 46.744450140634505\n",
      "After 1300 updates the Loss is: 44.95643749170373\n",
      "After 1400 updates the Loss is: 43.33918325602648\n",
      "After 1500 updates the Loss is: 41.8731420036575\n",
      "After 1600 updates the Loss is: 40.5411112457792\n",
      "After 1700 updates the Loss is: 39.32799454173159\n",
      "After 1800 updates the Loss is: 38.22056571760377\n",
      "After 1900 updates the Loss is: 37.20724515917424\n",
      "After 2000 updates the Loss is: 36.27789410794831\n",
      "After 2100 updates the Loss is: 35.42362951627235\n",
      "After 2200 updates the Loss is: 34.63665987697789\n",
      "After 2300 updates the Loss is: 33.91014117097286\n",
      "After 2400 updates the Loss is: 33.23805139034433\n",
      "After 2500 updates the Loss is: 32.61508178827191\n",
      "After 2600 updates the Loss is: 32.036542935655355\n",
      "After 2700 updates the Loss is: 31.498283730066497\n",
      "After 2800 updates the Loss is: 30.996621641551275\n",
      "After 2900 updates the Loss is: 30.528282651577452\n",
      "After 3000 updates the Loss is: 30.090349521566157\n"
     ]
    }
   ],
   "source": [
    "model = LR_rmsprop()\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cb4bc81-90ba-46f1-bde5-423f5440d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(y-model.predict_classes(x)))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea9d94-745c-4597-9967-ae3e14f5a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

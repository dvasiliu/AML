{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1fa758-6f6d-4d83-9031-53f35150a989",
   "metadata": {},
   "source": [
    "## Coding Applications with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb941c0-3691-4d5f-894b-6e5084c70f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, kstest, anderson, shapiro\n",
    "# for comparison only\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9f44e8-f3e2-4f78-a943-f81b1ac5882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/dvasiliu/AML/main/Data%20Sets/Advertising.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85dbde3-3945-4734-ac21-8d5df0d09e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the goal is to predict the Sales (cont. variable) by using values from TV, Radio and Newspaper advertising\n",
    "x = data.loc[:,['TV','Radio','Newspaper']].values\n",
    "y = data['Sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "273aae79-55c8-483f-ad5f-16ecd20208e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[230.1,  37.8,  69.2],\n",
       "       [ 44.5,  39.3,  45.1],\n",
       "       [ 17.2,  45.9,  69.3],\n",
       "       [151.5,  41.3,  58.5],\n",
       "       [180.8,  10.8,  58.4],\n",
       "       [  8.7,  48.9,  75. ],\n",
       "       [ 57.5,  32.8,  23.5],\n",
       "       [120.2,  19.6,  11.6],\n",
       "       [  8.6,   2.1,   1. ],\n",
       "       [199.8,   2.6,  21.2],\n",
       "       [ 66.1,   5.8,  24.2],\n",
       "       [214.7,  24. ,   4. ],\n",
       "       [ 23.8,  35.1,  65.9],\n",
       "       [ 97.5,   7.6,   7.2],\n",
       "       [204.1,  32.9,  46. ],\n",
       "       [195.4,  47.7,  52.9],\n",
       "       [ 67.8,  36.6, 114. ],\n",
       "       [281.4,  39.6,  55.8],\n",
       "       [ 69.2,  20.5,  18.3],\n",
       "       [147.3,  23.9,  19.1],\n",
       "       [218.4,  27.7,  53.4],\n",
       "       [237.4,   5.1,  23.5],\n",
       "       [ 13.2,  15.9,  49.6],\n",
       "       [228.3,  16.9,  26.2],\n",
       "       [ 62.3,  12.6,  18.3],\n",
       "       [262.9,   3.5,  19.5],\n",
       "       [142.9,  29.3,  12.6],\n",
       "       [240.1,  16.7,  22.9],\n",
       "       [248.8,  27.1,  22.9],\n",
       "       [ 70.6,  16. ,  40.8],\n",
       "       [292.9,  28.3,  43.2],\n",
       "       [112.9,  17.4,  38.6],\n",
       "       [ 97.2,   1.5,  30. ],\n",
       "       [265.6,  20. ,   0.3],\n",
       "       [ 95.7,   1.4,   7.4],\n",
       "       [290.7,   4.1,   8.5],\n",
       "       [266.9,  43.8,   5. ],\n",
       "       [ 74.7,  49.4,  45.7],\n",
       "       [ 43.1,  26.7,  35.1],\n",
       "       [228. ,  37.7,  32. ],\n",
       "       [202.5,  22.3,  31.6],\n",
       "       [177. ,  33.4,  38.7],\n",
       "       [293.6,  27.7,   1.8],\n",
       "       [206.9,   8.4,  26.4],\n",
       "       [ 25.1,  25.7,  43.3],\n",
       "       [175.1,  22.5,  31.5],\n",
       "       [ 89.7,   9.9,  35.7],\n",
       "       [239.9,  41.5,  18.5],\n",
       "       [227.2,  15.8,  49.9],\n",
       "       [ 66.9,  11.7,  36.8],\n",
       "       [199.8,   3.1,  34.6],\n",
       "       [100.4,   9.6,   3.6],\n",
       "       [216.4,  41.7,  39.6],\n",
       "       [182.6,  46.2,  58.7],\n",
       "       [262.7,  28.8,  15.9],\n",
       "       [198.9,  49.4,  60. ],\n",
       "       [  7.3,  28.1,  41.4],\n",
       "       [136.2,  19.2,  16.6],\n",
       "       [210.8,  49.6,  37.7],\n",
       "       [210.7,  29.5,   9.3],\n",
       "       [ 53.5,   2. ,  21.4],\n",
       "       [261.3,  42.7,  54.7],\n",
       "       [239.3,  15.5,  27.3],\n",
       "       [102.7,  29.6,   8.4],\n",
       "       [131.1,  42.8,  28.9],\n",
       "       [ 69. ,   9.3,   0.9],\n",
       "       [ 31.5,  24.6,   2.2],\n",
       "       [139.3,  14.5,  10.2],\n",
       "       [237.4,  27.5,  11. ],\n",
       "       [216.8,  43.9,  27.2],\n",
       "       [199.1,  30.6,  38.7],\n",
       "       [109.8,  14.3,  31.7],\n",
       "       [ 26.8,  33. ,  19.3],\n",
       "       [129.4,   5.7,  31.3],\n",
       "       [213.4,  24.6,  13.1],\n",
       "       [ 16.9,  43.7,  89.4],\n",
       "       [ 27.5,   1.6,  20.7],\n",
       "       [120.5,  28.5,  14.2],\n",
       "       [  5.4,  29.9,   9.4],\n",
       "       [116. ,   7.7,  23.1],\n",
       "       [ 76.4,  26.7,  22.3],\n",
       "       [239.8,   4.1,  36.9],\n",
       "       [ 75.3,  20.3,  32.5],\n",
       "       [ 68.4,  44.5,  35.6],\n",
       "       [213.5,  43. ,  33.8],\n",
       "       [193.2,  18.4,  65.7],\n",
       "       [ 76.3,  27.5,  16. ],\n",
       "       [110.7,  40.6,  63.2],\n",
       "       [ 88.3,  25.5,  73.4],\n",
       "       [109.8,  47.8,  51.4],\n",
       "       [134.3,   4.9,   9.3],\n",
       "       [ 28.6,   1.5,  33. ],\n",
       "       [217.7,  33.5,  59. ],\n",
       "       [250.9,  36.5,  72.3],\n",
       "       [107.4,  14. ,  10.9],\n",
       "       [163.3,  31.6,  52.9],\n",
       "       [197.6,   3.5,   5.9],\n",
       "       [184.9,  21. ,  22. ],\n",
       "       [289.7,  42.3,  51.2],\n",
       "       [135.2,  41.7,  45.9],\n",
       "       [222.4,   4.3,  49.8],\n",
       "       [296.4,  36.3, 100.9],\n",
       "       [280.2,  10.1,  21.4],\n",
       "       [187.9,  17.2,  17.9],\n",
       "       [238.2,  34.3,   5.3],\n",
       "       [137.9,  46.4,  59. ],\n",
       "       [ 25. ,  11. ,  29.7],\n",
       "       [ 90.4,   0.3,  23.2],\n",
       "       [ 13.1,   0.4,  25.6],\n",
       "       [255.4,  26.9,   5.5],\n",
       "       [225.8,   8.2,  56.5],\n",
       "       [241.7,  38. ,  23.2],\n",
       "       [175.7,  15.4,   2.4],\n",
       "       [209.6,  20.6,  10.7],\n",
       "       [ 78.2,  46.8,  34.5],\n",
       "       [ 75.1,  35. ,  52.7],\n",
       "       [139.2,  14.3,  25.6],\n",
       "       [ 76.4,   0.8,  14.8],\n",
       "       [125.7,  36.9,  79.2],\n",
       "       [ 19.4,  16. ,  22.3],\n",
       "       [141.3,  26.8,  46.2],\n",
       "       [ 18.8,  21.7,  50.4],\n",
       "       [224. ,   2.4,  15.6],\n",
       "       [123.1,  34.6,  12.4],\n",
       "       [229.5,  32.3,  74.2],\n",
       "       [ 87.2,  11.8,  25.9],\n",
       "       [  7.8,  38.9,  50.6],\n",
       "       [ 80.2,   0. ,   9.2],\n",
       "       [220.3,  49. ,   3.2],\n",
       "       [ 59.6,  12. ,  43.1],\n",
       "       [  0.7,  39.6,   8.7],\n",
       "       [265.2,   2.9,  43. ],\n",
       "       [  8.4,  27.2,   2.1],\n",
       "       [219.8,  33.5,  45.1],\n",
       "       [ 36.9,  38.6,  65.6],\n",
       "       [ 48.3,  47. ,   8.5],\n",
       "       [ 25.6,  39. ,   9.3],\n",
       "       [273.7,  28.9,  59.7],\n",
       "       [ 43. ,  25.9,  20.5],\n",
       "       [184.9,  43.9,   1.7],\n",
       "       [ 73.4,  17. ,  12.9],\n",
       "       [193.7,  35.4,  75.6],\n",
       "       [220.5,  33.2,  37.9],\n",
       "       [104.6,   5.7,  34.4],\n",
       "       [ 96.2,  14.8,  38.9],\n",
       "       [140.3,   1.9,   9. ],\n",
       "       [240.1,   7.3,   8.7],\n",
       "       [243.2,  49. ,  44.3],\n",
       "       [ 38. ,  40.3,  11.9],\n",
       "       [ 44.7,  25.8,  20.6],\n",
       "       [280.7,  13.9,  37. ],\n",
       "       [121. ,   8.4,  48.7],\n",
       "       [197.6,  23.3,  14.2],\n",
       "       [171.3,  39.7,  37.7],\n",
       "       [187.8,  21.1,   9.5],\n",
       "       [  4.1,  11.6,   5.7],\n",
       "       [ 93.9,  43.5,  50.5],\n",
       "       [149.8,   1.3,  24.3],\n",
       "       [ 11.7,  36.9,  45.2],\n",
       "       [131.7,  18.4,  34.6],\n",
       "       [172.5,  18.1,  30.7],\n",
       "       [ 85.7,  35.8,  49.3],\n",
       "       [188.4,  18.1,  25.6],\n",
       "       [163.5,  36.8,   7.4],\n",
       "       [117.2,  14.7,   5.4],\n",
       "       [234.5,   3.4,  84.8],\n",
       "       [ 17.9,  37.6,  21.6],\n",
       "       [206.8,   5.2,  19.4],\n",
       "       [215.4,  23.6,  57.6],\n",
       "       [284.3,  10.6,   6.4],\n",
       "       [ 50. ,  11.6,  18.4],\n",
       "       [164.5,  20.9,  47.4],\n",
       "       [ 19.6,  20.1,  17. ],\n",
       "       [168.4,   7.1,  12.8],\n",
       "       [222.4,   3.4,  13.1],\n",
       "       [276.9,  48.9,  41.8],\n",
       "       [248.4,  30.2,  20.3],\n",
       "       [170.2,   7.8,  35.2],\n",
       "       [276.7,   2.3,  23.7],\n",
       "       [165.6,  10. ,  17.6],\n",
       "       [156.6,   2.6,   8.3],\n",
       "       [218.5,   5.4,  27.4],\n",
       "       [ 56.2,   5.7,  29.7],\n",
       "       [287.6,  43. ,  71.8],\n",
       "       [253.8,  21.3,  30. ],\n",
       "       [205. ,  45.1,  19.6],\n",
       "       [139.5,   2.1,  26.6],\n",
       "       [191.1,  28.7,  18.2],\n",
       "       [286. ,  13.9,   3.7],\n",
       "       [ 18.7,  12.1,  23.4],\n",
       "       [ 39.5,  41.1,   5.8],\n",
       "       [ 75.5,  10.8,   6. ],\n",
       "       [ 17.2,   4.1,  31.6],\n",
       "       [166.8,  42. ,   3.6],\n",
       "       [149.7,  35.6,   6. ],\n",
       "       [ 38.2,   3.7,  13.8],\n",
       "       [ 94.2,   4.9,   8.1],\n",
       "       [177. ,   9.3,   6.4],\n",
       "       [283.6,  42. ,  66.2],\n",
       "       [232.1,   8.6,   8.7]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd0c58-4640-475c-8221-58d25d077c92",
   "metadata": {},
   "source": [
    "$$\\large Sales \\approx w_1*\\text{TV} + w_2*\\text{Radio} + w_3*{Newspaper} + w_0*1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34ffbeb0-18bc-4aa2-9ebc-f099813c4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we want to center the data, and then we want to find the optimal weights for a linear model, i.e. minimize the sum of squared errors\n",
    "# we may need a scaler\n",
    "def zscore(x):\n",
    "    return (x-np.mean(x,axis=0))/np.std(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e80b27-9d38-4a93-bb53-10a242d596d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xscaled = zscore(x)\n",
    "ycentered = y-np.mean(y) # this is only to show and tell, but it is not what we prefer in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f4ee66-8dd1-4c0b-96b0-fa3a9b837158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look for the optimal weights w1, w2 and w3 such that the predictions made with the corresponding linear combination of the columns is minimizing SSE\n",
    "# define the loss function\n",
    "# if we use ycentered we do not need an intercept term \n",
    "def L(w):\n",
    "    prediction = xscaled@w\n",
    "    errors = ycentered - prediction\n",
    "    return sum((errors**2))/len(errors) # this is MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a60fb16-4d10-4c83-95cf-12a8663b9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    prediction = xscaled@w\n",
    "    errors = ycentered - prediction\n",
    "    return (-1/len(errors))*errors@xscaled # here we applied the Chain Rule to differentiate; also the linear algebra behind matrix-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fcf72e4-7ab7-4e9f-abbb-36746fe836ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(18.62758321149871)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [1,2,3]\n",
    "L(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a73485-4034-4fb1-82c0-3fada2495584",
   "metadata": {},
   "source": [
    "### We want to improve w\n",
    "\n",
    "So, we apply an update in the direction of the negative gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "760f7ea1-9342-482c-8f7b-ea15f12f25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new = w - 0.1*gradient(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fb6f117-9e82-4244-b32c-2623168fd96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(15.876582967901031)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b79b3c01-bcf4-45ec-88ee-2157bbce41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "maxiter = 2000\n",
    "learning_rate = 0.01\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-6\n",
    "w_old = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2083321f-0d62-4324-92f6-28b77507822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 5.529280549953874\n",
      "The MSE is : 3.3339875890418336\n",
      "The MSE is : 2.907894189771724\n",
      "The MSE is : 2.814370267921421\n",
      "The MSE is : 2.791904755916669\n",
      "The MSE is : 2.7861861325459163\n",
      "The MSE is : 2.7846804566616536\n",
      "The MSE is : 2.784276631451753\n",
      "The MSE is : 2.7841672639684334\n",
      "The MSE is : 2.7841374943577\n",
      "The MSE is : 2.7841293701756977\n",
      "The MSE is : 2.7841271501531195\n",
      "The MSE is : 2.7841265431019444\n",
      "The MSE is : 2.784126377051365\n",
      "The algorithm has converged!\n"
     ]
    }
   ],
   "source": [
    "# how to implement the gradient descent\n",
    "for it in range(maxiter):\n",
    "    w_new = w_old - learning_rate*gradient(w_old)\n",
    "\n",
    "    if max(abs(w_new-w_old))<tol:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break\n",
    "\n",
    "    w_old = w_new\n",
    "    if (it+1)%100==0:\n",
    "        print('The MSE is : '+str(L(w_old)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e9239b0-b7eb-4ef8-8f2d-73900029fc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.91925106,  2.79190903, -0.02238423])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_old # these are the found optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fd227b8-3212-492a-90a7-0edd71368314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.91925365,  2.79206274, -0.02253861])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(xscaled, ycentered)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6068cc71-c959-4f4a-802c-e7549526c519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.784126314510936)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((ycentered - model.predict(xscaled))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11e684bc-8939-4dce-8e54-e7acf0f88cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.7841263451716043)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(w_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d89bbb1-9e3d-46c5-982a-7bf3cd3059c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat the gradient descent by including a bias term and by NOT centereing the values of y\n",
    "x_aug = np.column_stack([np.ones(len(xscaled)),xscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9df6b999-6b85-4bd7-9e14-7e875ee5466d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969852</td>\n",
       "      <td>0.981522</td>\n",
       "      <td>1.778945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.197376</td>\n",
       "      <td>1.082808</td>\n",
       "      <td>0.669579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.516155</td>\n",
       "      <td>1.528463</td>\n",
       "      <td>1.783549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052050</td>\n",
       "      <td>1.217855</td>\n",
       "      <td>1.286405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394182</td>\n",
       "      <td>-0.841614</td>\n",
       "      <td>1.281802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.270941</td>\n",
       "      <td>-1.321031</td>\n",
       "      <td>-0.771217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.617035</td>\n",
       "      <td>-1.240003</td>\n",
       "      <td>-1.033598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349810</td>\n",
       "      <td>-0.942899</td>\n",
       "      <td>-1.111852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.594565</td>\n",
       "      <td>1.265121</td>\n",
       "      <td>1.640850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993206</td>\n",
       "      <td>-0.990165</td>\n",
       "      <td>-1.005979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3\n",
       "0    1.0  0.969852  0.981522  1.778945\n",
       "1    1.0 -1.197376  1.082808  0.669579\n",
       "2    1.0 -1.516155  1.528463  1.783549\n",
       "3    1.0  0.052050  1.217855  1.286405\n",
       "4    1.0  0.394182 -0.841614  1.281802\n",
       "..   ...       ...       ...       ...\n",
       "195  1.0 -1.270941 -1.321031 -0.771217\n",
       "196  1.0 -0.617035 -1.240003 -1.033598\n",
       "197  1.0  0.349810 -0.942899 -1.111852\n",
       "198  1.0  1.594565  1.265121  1.640850\n",
       "199  1.0  0.993206 -0.990165 -1.005979\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85b7c8ed-ef4d-44a0-a392-a04877f7967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "maxiter = 2000\n",
    "learning_rate = 0.01\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-6\n",
    "w_old = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "182bbdab-aa4d-4492-ab87-9ec2a839bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(w):\n",
    "    prediction = x_aug@w\n",
    "    errors = y - prediction\n",
    "    return sum((errors**2))/len(errors) # this is MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1f8e5e9d-c305-40b3-843d-40c447829b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    prediction = x_aug@w\n",
    "    errors = y - prediction\n",
    "    return (-2/len(errors))*errors@x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a94f3298-bc27-47ae-8b22-c85f69a1e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 6.785431261566528\n",
      "The MSE is : 2.8746070104389174\n",
      "The MSE is : 2.787202061544548\n",
      "The MSE is : 2.784290392040183\n",
      "The MSE is : 2.7841373612765046\n",
      "The MSE is : 2.7841271146194715\n",
      "The MSE is : 2.784126373560019\n",
      "The algorithm has converged!\n"
     ]
    }
   ],
   "source": [
    "# how to implement the gradient descent\n",
    "for it in range(maxiter):\n",
    "    w_new = w_old - learning_rate*gradient(w_old)\n",
    "\n",
    "    if max(abs(w_new-w_old))<tol:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break\n",
    "\n",
    "    w_old = w_new\n",
    "    if (it+1)%100==0:\n",
    "        print('The MSE is : '+str(L(w_old)))\n",
    "w_trained = w_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c4e0b2c0-629b-447f-84ad-a12693305ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the weights for prediction\n",
    "def predict(w,x):\n",
    "    return x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f750884-e8b8-43ce-bdb8-35162434ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,predicted):\n",
    "    return 1 - sum((y-predicted)**2)/sum((y-np.mean(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2cab3fd5-0936-48d9-a399-8731754617e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.897210637899863)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the R2-score of our predicted values is\n",
    "score(y,predict(w_old,x_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959686b-e47a-4010-98b9-2639acee7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make a 5-Fold CV\n",
    "permuted_ind = np.random.permutation(range(len(x)))\n",
    "folds = np.array_split(permuted_ind,5)\n",
    "\n",
    "for fold in folds:\n",
    "    testind = fold\n",
    "    trainind = np.delete(range(len(x)),fold)\n",
    "    xtrain = x_aug[trainind]\n",
    "    xtest = x_aug[testind]\n",
    "    # here you update the weights in the train data and then you can predict on test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a1a3a-421f-44a8-8def-b7d9f2159ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39efd5af-8fee-4009-8aaa-591c14ddd125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.0225    ,  -2.67998863,   1.47233287,   3.93080046])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient([0,1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5038a43-5420-46ba-9c0d-bf9a4e8f50f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40225   , 1.26799886, 2.85276671, 3.60691995])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0,1,3,4] - 0.1*gradient([0,1,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c28b5f-908e-4eef-8352-9653aa4bf5bd",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Main idea: we want to minimize the MSE with a constraint on the weights, such as sum of squared weight is less than something -> this is equivalent to the following objective \n",
    "\n",
    "$$\\text{L-ridge}(w):= MSE + \\alpha*\\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "Critical thinking question: what is the gradient of the Ridge objective function?\n",
    "\n",
    "$$\\text{gradient-ridge}(w):= \\nabla MSE + 2*\\alpha*w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9f8f462a-bde0-451e-b152-2265515c9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_ridge(w):\n",
    "    prediction = x_aug@w \n",
    "    errors = y - prediction\n",
    "    return sum((errors**2))/len(errors) + alpha*sum(w**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f999981d-b998-44df-92d8-01f10c9e9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ridge(w):\n",
    "    prediction = x_aug@w\n",
    "    errors = y - prediction\n",
    "    return (-2/len(errors))*errors@x_aug + 2*alpha*np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1df4771-9eda-4776-aac5-de88c5fc047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.0225    ,  -2.60334075,   1.88643662,   5.03080046])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient([0,1,3,5]) + 2*0.01*np.array([0,1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc40e9fb-d9d3-416a-8d66-9dd8e70dd474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply GD for training the Ridge coefficients\n",
    "# we want the \"optimal\" value for the weights\n",
    "maxiter = 2000\n",
    "learning_rate = 0.01\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-6\n",
    "w_old = [0,1,2,3]\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "08034bbe-5c90-420b-b543-de58a52ecc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 7.1249749225194785\n",
      "The MSE is : 2.963671733638895\n",
      "The MSE is : 2.8193454219648384\n",
      "The MSE is : 2.807547251958973\n",
      "The MSE is : 2.8060100860177983\n",
      "The MSE is : 2.8057644119668\n",
      "The MSE is : 2.805718529521846\n",
      "The algorithm has converged!\n"
     ]
    }
   ],
   "source": [
    "# how to implement the gradient descent\n",
    "for it in range(maxiter):\n",
    "    w_new = w_old - learning_rate*gradient_ridge(w_old)\n",
    "\n",
    "    if max(abs(w_new-w_old))<tol:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break\n",
    "\n",
    "    w_old = w_new\n",
    "    if (it+1)%100==0:\n",
    "        print('The MSE is : '+str(L(w_old)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823804b8-71d3-4747-a9d5-0d6d0a67b64c",
   "metadata": {},
   "source": [
    "## The Mini_Batch Stochastic Gradient Descent\n",
    "\n",
    "Not getting stuck in a sub-optimal local minima!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cacace0-c439-47aa-85be-3f0de2f4f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one shuffle the data (randomly permute the order of observations)\n",
    "# step two split it in mini-batches (subsets of pretty much the same cardinality)\n",
    "# for updating the weights we only use gradients computed on the mini-batches\n",
    "####################################\n",
    "\n",
    "# step 1:\n",
    "ind = np.random.permutation(range(len(x)))\n",
    "batches = np.array_split(ind,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a753f00-950f-4188-87a3-f0159b85bdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([139,  14,  34,  55, 107, 166, 148, 167,   9, 109, 129,  81,  94,\n",
       "        132,  21,  87, 195,  88,  53, 174]),\n",
       " array([ 44,  45,  42,  11,  82, 180,  98,  86, 170, 134, 146,  24,   0,\n",
       "         36,   7, 100, 171,  27, 115,  65]),\n",
       " array([198, 183, 112, 121,  64, 190, 138, 179, 122, 114,  32, 144,  63,\n",
       "         69,  10,   6, 137, 176,  38, 157]),\n",
       " array([ 99,   5, 156,  73,  12, 177, 186, 196,  85, 154,  46, 101,  23,\n",
       "         66,  47,  78,  30,  74,  95,  52]),\n",
       " array([163, 123,  50,  89, 191,  83, 187,  80, 155, 142, 143, 136, 188,\n",
       "        130,  79, 125, 104, 105,   3,   4]),\n",
       " array([  8, 106, 192, 165, 145, 150, 119, 153,  56,  35,  19, 117, 152,\n",
       "         33,  13,  77, 120, 168, 111,  75]),\n",
       " array([ 28, 128, 149, 199,  40, 161,  72, 116, 184,  84,  29,  39, 197,\n",
       "         20,  70, 102, 147, 126, 124,  31]),\n",
       " array([135, 162, 133, 169, 158, 175,  96, 181,  48,  90, 118,  57, 151,\n",
       "        189, 140,   2,  17, 182,  18, 103]),\n",
       " array([ 67,  76,  59,  62,  15, 159, 110,  51, 164,  25, 127,  16, 131,\n",
       "          1, 108, 185,  97,  60,  91,  43]),\n",
       " array([194, 173,  41, 172,  92,  22,  68, 113,  58, 141, 160, 178,  71,\n",
       "         54,  93,  61,  26,  37, 193,  49])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a11b1045-ef87-451b-89ff-f0f7b28c5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w,ind):\n",
    "    prediction = x_aug[ind]@w\n",
    "    errors = y[ind] - prediction\n",
    "    return (-2/len(errors))*errors@x_aug[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2389edef-d72d-4e4c-b4cd-bfa64097bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "epochs = 4000\n",
    "batch_size = 12\n",
    "learning_rate = 0.01\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-5\n",
    "w_old = [0,1,2,3]\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "27d9d47c-6c94-4d3d-a000-9f16748530b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 2.7842189191833615\n",
      "The MSE is : 2.78416943127234\n",
      "The MSE is : 2.784258876384771\n",
      "The MSE is : 2.784263639723103\n",
      "The MSE is : 2.7841842989728742\n",
      "The MSE is : 2.784252372954826\n",
      "The MSE is : 2.784195779044922\n",
      "The MSE is : 2.784133676544374\n",
      "The MSE is : 2.7841602415484603\n",
      "The MSE is : 2.784369877212638\n",
      "The MSE is : 2.784423654631977\n",
      "The MSE is : 2.7843352208502727\n",
      "The MSE is : 2.7842152038733947\n",
      "The MSE is : 2.7841539779405595\n",
      "The MSE is : 2.784231697251457\n",
      "The MSE is : 2.784231089111365\n",
      "The MSE is : 2.7843871817616925\n",
      "The MSE is : 2.7845327257215975\n",
      "The MSE is : 2.784135369795406\n",
      "The MSE is : 2.7843243943619367\n"
     ]
    }
   ],
   "source": [
    "# how to implement the gradient descent\n",
    "for epoch in range(epochs):\n",
    "    ind = np.random.permutation(range(len(x)))\n",
    "    batches = np.array_split(ind,batch_size)\n",
    "    for ind in batches:\n",
    "        w_new = w_old - learning_rate*gradient(w_old,ind)\n",
    "    \n",
    "        if max(abs(w_new-w_old))<tol:\n",
    "            done = True\n",
    "            break\n",
    "    \n",
    "        w_old = w_new\n",
    "    if (epoch+1)%200==0:\n",
    "        print('The MSE is : '+str(L(w_old)))\n",
    "    if done:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d33dc3-3072-40cb-888b-f8ff290269c9",
   "metadata": {},
   "source": [
    "### Mini-Batch Stochastic Adaptive Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bfe12ed0-d700-4fb3-936e-993002448d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "epochs = 5000\n",
    "batch_size = 10\n",
    "learning_rate = 0.05\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-6\n",
    "w_old = [0,1,2,3]\n",
    "done = False\n",
    "eta = 1\n",
    "epsilon = 1e-6\n",
    "s = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e41298c-7de1-4096-8e4c-581f1418f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 2.7841754289097493\n",
      "The MSE is : 2.7841290799417724\n",
      "The MSE is : 2.784133302795261\n",
      "The MSE is : 2.784128055301857\n",
      "The MSE is : 2.784127971234982\n",
      "The MSE is : 2.784130563780534\n",
      "The MSE is : 2.784126765684692\n",
      "The MSE is : 2.7841268391587852\n",
      "The MSE is : 2.784127930945166\n",
      "The MSE is : 2.7841266169300978\n"
     ]
    }
   ],
   "source": [
    "# how to implement the mini-batch adaptive gradient descent\n",
    "for epoch in range(epochs):\n",
    "    ind = np.random.permutation(range(len(x)))\n",
    "    batches = np.array_split(ind,len(x)//batch_size)\n",
    "    for ind in batches:\n",
    "        g = gradient(w_old,ind)\n",
    "        s += sum(g**2)\n",
    "        learning_rate = eta/np.sqrt(s+epsilon)\n",
    "        w_new = w_old - learning_rate*g\n",
    "    \n",
    "        if max(abs(w_new-w_old))<tol:\n",
    "            done = True\n",
    "            break\n",
    "    \n",
    "        w_old = w_new\n",
    "    if (epoch+1)%500==0:\n",
    "        print('The MSE is : '+str(L(w_old)))\n",
    "    if done:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3d2c9-96b7-4d04-adc4-0a852ef9dcfa",
   "metadata": {},
   "source": [
    "## Root Mean Square Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "31755103-a607-4514-8a40-0357bb0c3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "epochs = 10000\n",
    "batch_size = 100\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-5\n",
    "w_old = [0,1,2,3]\n",
    "done = False\n",
    "eta = 0.001\n",
    "epsilon = 1e-8\n",
    "s = 0\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d04264f-7ea8-439e-89d1-443c844e3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 187.16783046873715\n",
      "The MSE is : 161.30120279966025\n",
      "The MSE is : 137.39586115456973\n",
      "The MSE is : 115.44982169433682\n",
      "The MSE is : 95.47493297226922\n",
      "The MSE is : 77.44858352379518\n",
      "The MSE is : 61.383127201626266\n",
      "The MSE is : 47.273506861970766\n",
      "The MSE is : 35.11536246326244\n",
      "The MSE is : 24.905693842639447\n",
      "The MSE is : 16.65320218281837\n",
      "The MSE is : 10.336073223499197\n",
      "The MSE is : 5.95635919710087\n",
      "The MSE is : 3.490003669498391\n",
      "The MSE is : 2.7992911637772555\n",
      "The MSE is : 2.7842085359018456\n",
      "The MSE is : 2.784127047456139\n",
      "The MSE is : 2.7841264635002965\n",
      "The MSE is : 2.7841263759536723\n",
      "The MSE is : 2.784126505514004\n"
     ]
    }
   ],
   "source": [
    "# how to implement the mini-batch adaptive gradient descent\n",
    "for epoch in range(epochs):\n",
    "    ind = np.random.permutation(range(len(x)))\n",
    "    batches = np.array_split(ind,len(x)//batch_size)\n",
    "    for ind in batches:\n",
    "        g = gradient(w_old,ind)\n",
    "        s = gamma*s +(1-gamma)*sum(g**2)\n",
    "        learning_rate = eta/np.sqrt(s+epsilon)\n",
    "        w_new = w_old - learning_rate*g\n",
    "    \n",
    "        if max(abs(w_new-w_old))<tol:\n",
    "            done = True\n",
    "            break\n",
    "    \n",
    "        w_old = w_new\n",
    "    if (epoch+1)%500==0:\n",
    "        print('The MSE is : '+str(L(w_old)))\n",
    "    if done:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc73f5-ca53-4fa4-b8eb-92d942283ff8",
   "metadata": {},
   "source": [
    "## Adaptive Momentum Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6db2af48-1afc-457e-a565-5deb983e64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the \"optimal\" value for the weights\n",
    "epochs = 10000\n",
    "batch_size = 30\n",
    "# we want some stopping criteria, for example if the weights are no longer significantly updated\n",
    "# we measure the difference between w_new and w_old vs epsilon -> tolerance for deciding convergence.\n",
    "tol = 1e-6\n",
    "w_old = [0,1,2,3]\n",
    "done = False\n",
    "eta = 0.001\n",
    "epsilon = 1e-8\n",
    "s = 0\n",
    "v = 0\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "581f502c-94b5-4b1a-adab-dcf3afd0cbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 93.44796348843207\n",
      "The MSE is : 46.892041949610764\n",
      "The MSE is : 17.04842817145521\n",
      "The MSE is : 3.8496944871446677\n",
      "The MSE is : 2.784696365984472\n",
      "The MSE is : 2.784126878435726\n",
      "The MSE is : 2.7841282937753236\n",
      "The MSE is : 2.784127202026523\n",
      "The MSE is : 2.7841274494362365\n",
      "The MSE is : 2.784126706362431\n",
      "The MSE is : 2.7841264882160384\n",
      "The MSE is : 2.784126784530232\n",
      "The MSE is : 2.7841263271015246\n",
      "The MSE is : 2.784127178511048\n",
      "The MSE is : 2.7841266744302784\n",
      "The MSE is : 2.784126422310194\n",
      "The MSE is : 2.7841264642137316\n",
      "The MSE is : 2.784126654006661\n",
      "The MSE is : 2.784126724344282\n",
      "The MSE is : 2.78412646665901\n"
     ]
    }
   ],
   "source": [
    "# how to implement the mini-batch adaptive gradient descent\n",
    "for epoch in range(epochs):\n",
    "    ind = np.random.permutation(range(len(x)))\n",
    "    batches = np.array_split(ind,len(x)//batch_size)\n",
    "    for ind in batches:\n",
    "        g = gradient(w_old,ind)\n",
    "        v = beta1*v + (1-beta1)*g\n",
    "        s = beta2*s +(1-beta2)*sum(g**2)\n",
    "        v = v/(1-beta1**(t+1))\n",
    "        s = s/(1-beta2**(t+1))\n",
    "        learning_rate = eta/(np.sqrt(s)+epsilon)\n",
    "        w_new = w_old - learning_rate*v\n",
    "        \n",
    "        if max(abs(w_new-w_old))<tol:\n",
    "            done = True\n",
    "            break\n",
    "        t += 1\n",
    "        w_old = w_new\n",
    "    if (epoch+1)%500==0:\n",
    "        print('The MSE is : '+str(L(w_old)))\n",
    "    if done:\n",
    "        print(\"The algorithm has converged!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5801e-2199-4915-9f1a-fd3ed8959d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

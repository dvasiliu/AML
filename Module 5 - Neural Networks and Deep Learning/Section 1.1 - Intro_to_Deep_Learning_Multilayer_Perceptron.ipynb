{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D_g8i1kdovH"
   },
   "source": [
    "## <font color='blue' size=6pt> An Introduction to the Multilayer Perceptron </font>\n",
    "\n",
    "### <font color='blue' size=5pt> Introduction </font>\n",
    "\n",
    "The related research started in late 1950s ([F. Rosenblatt](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)) with the “Mark I Perceptron”: the goal was to automatically detect capital letters.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "    <img src=\"https://i.imgur.com/Fo4Mu7V.png\" alt=\"Image 1\"\n",
    "    width='700px'> </td>\n",
    "   <td>  \n",
    "   <img src=\"https://i.imgur.com/tL8bXYl.png\" alt=\"Image 2\" width='500px'> </td>\n",
    "</tr></table>\n",
    "\n",
    "Image source: https://jennysmoore.wordpress.com/2014/03/31/march-31-network-society-readings/\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> The Artificial Neuron </font>\n",
    "\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "    <img src=\"https://i.imgur.com/JKHqlVt.png\" alt=\"Image 1\"\n",
    "    width='600px'> </td>\n",
    "   <td>  \n",
    "   <img src=\"https://i.imgur.com/EtLULhh.png\" alt=\"Image 2\" width='700px'> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Example of a neural network with five neurons:\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "    <img src=\"https://i.imgur.com/NQLc0B4.png\" alt=\"Image 1\"\n",
    "    width='800px height=800px'> </td>\n",
    "   <td>  \n",
    "   <img src=\"https://i.imgur.com/XipowyJ.png\" alt=\"Image 2\" width='640px'> </td>\n",
    "</tr></table>\n",
    "\n",
    "It is a nature-inspired design. Check out the [playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.61321&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
    "\n",
    "**Fact:** A frog has enough neurons to learn how to drive your car, but if it did, it would occupy its entire memory and it would not know how to feed itself.\n",
    "\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "    <img src=\"https://i.imgur.com/so9Z0VY.png\" alt=\"Image 1\"\n",
    "    width='300px'> </td>\n",
    "   <td>  \n",
    "   <img src=\"https://i.imgur.com/N6Rc9n2.png\" alt=\"Image 2\" width='250px'> </td>\n",
    "</tr></table>\n",
    "\n",
    "### <font color='blue' size=5pt> Brief History of Development </font>\n",
    "\n",
    "Neural Networks have been a success for computer vision, image analysis and classification problems; however we can use the method for regression, as well.\n",
    "\n",
    " -  It was able to identify capital letters.\n",
    "\n",
    " -  From 1960 - 1986, things progressed relatively slowly.\n",
    "\n",
    " -  D. Rumelhart, [G. Hinton](https://torontolife.com/tech/ai-superstars-google-facebook-apple-studied-guy/) and R. Williams were able to achieve the first back-propagation network (Seminal paper “Learning Representations by back-propagating errors” Nature Vol. 323, 1986 ).\n",
    "\n",
    " -  1990, P. Werbos, “Backpropagation Through Time: What It Does and How to Do It”\n",
    "\n",
    " -  G. Hinton and R. Salakhutdinov examined the capability of neural nets for image recognition in 2006. Explosive research into neural nets from 2006 - today.\n",
    "\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> Activation Functions </font>\n",
    "\n",
    "Examples of activation functions:\n",
    "\n",
    "<style>\n",
    "img {width: 10%;}\n",
    "table th:first-of-type {\n",
    "    width: 10%;\n",
    "}\n",
    "table th:nth-of-type(2) {\n",
    "    width: 10%;\n",
    "}\n",
    "table th:nth-of-type(3) {\n",
    "    width: 40%;\n",
    "}\n",
    "table th:nth-of-type(4) {\n",
    "    width: 40%;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "|Name                                   |Plot                                        | Equation                                             | Derivative                                 |\n",
    "|----------------------------------------|---------------------------------------------|------------------------------------------------------|--------------------------------------------|\n",
    "|Identity                               | ![Identity](https://i.imgur.com/OZZutkF.png)           | $$ f(x) = x $$                                       | $$ f'(x) = 1 $$                            |\n",
    "| Binary step                            | ![Binary step](https://i.imgur.com/zRVpQIU.png)      | $$ f(x) = \\begin{cases} 0 & \\text{for } x < 0 \\\\ 1 & \\text{for } x \\ge 0 \\end{cases} $$ | $$ f'(x) = \\begin{cases} 0 & \\text{for } x \\ne 0 \\\\ ? & \\text{for } x = 0 \\end{cases} $$ |\n",
    "| Logistic (a.k.a Soft step)             | ![Logistic](https://i.imgur.com/VtjWCF6.png)            | $$ f(x) = \\frac{1}{1 + e^{-x}} $$                    | $$ f'(x) = f(x)(1 - f(x)) $$               |\n",
    "| TanH                                   | ![TanH](https://i.imgur.com/F7dSp0r.png)                    | $$ f(x) = \\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1 $$    | $$ f'(x) = 1 - f(x)^2 $$                   |\n",
    "| ArcTan                                 | ![ArcTan](https://i.imgur.com/gY6mIAO.png)               | $$ f(x) = \\tan^{-1}(x) $$                            | $$ f'(x) = \\frac{1}{x^2 + 1} $$            |\n",
    "| Rectified Linear Unit (ReLU)           | ![ReLU](https://i.imgur.com/GFNymDd.png)                    | $$ f(x) = \\begin{cases} 0 & \\text{for } x < 0 \\\\ x & \\text{for } x \\ge 0 \\end{cases} $$ | $$ f'(x) = \\begin{cases} 0 & \\text{for } x < 0 \\\\ 1 & \\text{for } x \\ge 0 \\end{cases} $$ |\n",
    "| Parametric Rectified Linear Unit (PReLU) | ![PReLU](https://i.imgur.com/UkIefvc.png)              | $$ f(x) = \\begin{cases} \\alpha x & \\text{for } x < 0 \\\\ x & \\text{for } x \\ge 0 \\end{cases} $$ | $$ f'(x) = \\begin{cases} \\alpha & \\text{for } x < 0 \\\\ 1 & \\text{for } x \\ge 0 \\end{cases} $$ |\n",
    "| Exponential Linear Unit (ELU)          | ![ELU](https://i.imgur.com/C5Qbkak.png)                      | $$ f(x) = \\begin{cases} \\alpha (e^x - 1) & \\text{for } x < 0 \\\\ x & \\text{for } x \\ge 0 \\end{cases} $$ | $$ f'(x) = \\begin{cases} f(x) + \\alpha & \\text{for } x < 0 \\\\ 1 & \\text{for } x \\ge 0 \\end{cases} $$ |\n",
    "| SoftPlus                               | ![SoftPlus](https://i.imgur.com/nzGthI4.png)            | $$ f(x) = \\log(1 + e^x) $$                           | $$ f'(x) = \\frac{1}{1 + e^{-x}} $$         |\n",
    "| Swish | ![Swish](https://i.imgur.com/55GMfys.png) | $$f(x) = \\frac{x}{1+e^-{\\beta x}}$$ | $$f'(x)=f(x)\\cdot\\left[1+\\beta - \\beta\\cdot\\sigma(\\beta x)\\right]$$ where $\\sigma$ is the logistic sigmoid. |\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> Backpropagation </font>\n",
    "\n",
    "Backpropagation is the process of updating the weights in a direction that is based on the calculation of the gradient of the loss function.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "    <img src=\"https://i.imgur.com/ea5kWB3.png\" alt=\"Image 1\"\n",
    "    width='410px'> </td>\n",
    "   <td>  \n",
    "   <img src=\"https://i.imgur.com/ySyoqTv.png\" alt=\"Image 2\" width='620px'> </td>\n",
    "</tr></table>\n",
    "\n",
    "### <font color='blue' size=5pt> Possible Issues </font>\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/cGoqAow.png\" width='500px'/>\n",
    "<figcaption>The Vanishing Gradient Problem</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "### <font color='blue' size=5pt> Criticism </font>\n",
    "\n",
    "[Reference Paper](https://www.science.org/content/article/ai-researchers-allege-machine-learning-alchemy)\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/omHzaNr.png\" width='500px'/>\n",
    "<figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> Gradient Descent Methods for Optimization </font>\n",
    "\n",
    "All the current optimization algorithms are based on a variant of gradient descent.\n",
    "\n",
    "Let's denote the vector of weights at step $t$ by $w_t$ and the gradient of the objective function with respect to the weights by $g_t$. The idea is that the gradient descent algorithm updates the weights under the following principle:\n",
    "\n",
    "$$\\large w_t = w_{t-1} - \\eta\\cdot g_{t,t-1}$$\n",
    "\n",
    "When the objective function (whose gradient with respect to the weights) is represented by $g_t$ and has multiple local minima, or it has a very shallow region containing the minima, the plain gradient descent algorithm may not converge to the position sought for. To remediate this deficiency research proposed alternatives by varying the way we evaluate the learning rate each step or how we compute the \"velocity\" for updating the weights:\n",
    "\n",
    "<font color='green'>$$\\Large w_t = w_{t-1} - \\eta_t\\cdot v_t$$ </font>\n",
    "\n",
    "In the equation above, $\\eta_t$ is an adaptive learning rate and $v_t$ a modified gradient.\n",
    "\n",
    "### <font color='blue' size=5pt> Dynamic Learning Rates </font>\n",
    "\n",
    "We can consider an exponential decay, such as\n",
    "\n",
    "$$\\large \\eta_t = \\eta_0 e^{-\\lambda\\cdot t}$$\n",
    "\n",
    "or a polynomial decay\n",
    "\n",
    "$$\\large \\eta_t = \\eta_0 (\\beta t+1)^{-\\alpha}$$\n",
    "\n",
    "### <font color='blue' size=5pt> Momentum Gradient Descent </font>\n",
    "\n",
    "$$\\large g_{t,t-1} = \\partial_w \\frac{1}{|\\text{B}_t|}\\sum_{i\\in \\text{B}_t}f(x_i,w_{t-1})=\\frac{1}{|\\text{B}_t|}\\sum_{i\\in \\text{B}_t}h_{i,t-1}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\large v_t = \\beta v_{t-1} + g_{t,t-1}$$\n",
    "\n",
    "and $\\beta\\in (0,1).$\n",
    "\n",
    "For an explicit formula, we have\n",
    "\n",
    "$$\\large v_t = \\sum_{\\tau=0}^{t-1} \\beta^{\\tau}g_{t-\\tau,t-\\tau-1}$$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$$\\large w_t = w_{t-1} - \\alpha v_t$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\large \\alpha = \\frac{\\eta}{1-\\beta}$$\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> AdaGrad (Adaptive Gradient Descent) </font>\n",
    "\n",
    "$$\\large s_t = s_{t-1} + g_{t}^2$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\large w_t= w_{t-1} - \\frac{\\eta}{\\sqrt{s_t+\\epsilon}}\\cdot g_t$$\n",
    "\n",
    "### <font color='blue' size=5pt> RMSProp (Root Mean Square Propagation) </font>\n",
    "\n",
    "$$\\large s_t = \\gamma\\cdot s_{t-1} + (1-\\gamma)\\cdot g_{t}^2$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\large w_t= w_{t-1} - \\frac{\\eta}{\\sqrt{s_t+\\epsilon}}\\cdot g_t$$\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "$$\\large s_t = (1-\\gamma)\\cdot (g_t^2+\\gamma g_{t-1}^2+\\gamma^2 g_{t-2} +\\gamma^3 g_{t-2} + ... + \\gamma^{\\tau} g_0)$$\n",
    "\n",
    "### <font color='blue' size=5pt> ADAM (Adaptive Momentum Gradient Descent) </font>\n",
    "\n",
    "$$\\large v_t = \\beta_1  v_{t-1} +(1-\\beta_1) g_t$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\large s_t = \\beta_2 s_{t-1} + (1-\\beta_2) g_t^2 $$\n",
    "\n",
    "We further consider\n",
    "\n",
    "$$\\large \\hat{v}_t = \\frac{v_t}{1-\\beta_1}, \\text{and  } \\hat{s}_t = \\frac{s_t}{1-\\beta_2} $$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$$\\large \\hat{g}_{t} = \\frac{\\eta\\cdot \\hat{v}_t}{\\sqrt{\\hat{s}_t}+\\epsilon}$$\n",
    "\n",
    "The updates to the weights are implemented as follows\n",
    "\n",
    "$$\\large w_t = w_{t-1} - \\hat{g}_t$$\n",
    "\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> Multilayer Perceptron Instructional Videos </font>\n",
    "\n",
    "The first 2 are required. The last 2 are optional but highly recommended, even if you have not had any calculus or linear algebra!\n",
    "\n",
    "1. [But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "2. [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "\n",
    "3. [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n",
    "4. [Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "\n",
    "### <font color='blue' size=5pt> Test Your Understanding </font>\n",
    "\n",
    "1. What is a multilayer perceptron?\n",
    "2. What is a hidden layer?\n",
    "3. The network in the video was on the small-ish side, having only 2 hidden layers with 16 neurons each.  How many total parameters (i.e. weights and biases) have to be determined during the training process for this network?\n",
    "4. Without reference to the calculus involved, do you understand the concept of gradient descent?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zRyus4-un7J"
   },
   "source": [
    "## <font color='blue' size=6pt> Code Applications </font>\n",
    "\n",
    "---\n",
    "\n",
    "### References:\n",
    "\n",
    "1. [Programming PyTorch for Deep Learning](https://www.amazon.com/Programming-PyTorch-Deep-Learning-Applications/dp/1492045357?asc_campaign=343db101986d123592617b298c3e663c&asc_source=01HPSC557H8MR8GX011W62TB1D&tag=snx192-20)\n",
    "2. [Getting Things Done with PyTorch](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aunr_fxE2ABj"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Cpnj3zsDunPv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# PyTorch coding expects data to be put in specific objects\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# here we just get some real data and preprocessing\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6EUDXKQ2GnM"
   },
   "source": [
    "### Example of a Neural Net in PyTorch for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ngkIKLOzWzhm"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTQicKbgW0cm",
    "outputId": "e008995a-7ef3-4d63-c4dc-69a7deadc980"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jTEr9KCXQ3t",
    "outputId": "0adb8ca1-ada2-44ab-bbf8-06619a3039cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5BjXLsqbCVP",
    "outputId": "86ed503a-c08e-4812-e1e1-5c9797fe7f6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer needs 14*5 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5N9m-fsmSYD",
    "outputId": "2597ae1a-7fd5-4a32-bfea-0174025f4dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0032\n",
      "Epoch [20/100], Loss: 0.0003\n",
      "Epoch [30/100], Loss: 0.0001\n",
      "Epoch [40/100], Loss: 0.0001\n",
      "Epoch [50/100], Loss: 0.0002\n",
      "Epoch [60/100], Loss: 0.0000\n",
      "Epoch [70/100], Loss: 0.0001\n",
      "Epoch [80/100], Loss: 0.0000\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "Accuracy on test set: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Standardize the features\n",
    "scale = StandardScaler()\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=301)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(scale.fit_transform(X_train), dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(scale.transform(X_test), dtype=torch.float64)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "# Define a simple neural network\n",
    "class WineNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(WineNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 16).double()\n",
    "        self.a1 = nn.PReLU().double()  # Define PReLU as a class member\n",
    "        self.hl2 = nn.Linear(16,8).double()\n",
    "        self.a2 = nn.GELU().double()\n",
    "        self.fc2 = nn.Linear(8, 3).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.fc1(x))  # Apply PReLU activation\n",
    "        x = self.a2(self.hl2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = WineNet(X_train.shape[1])\n",
    "# this criterion is based on the type of problem you are solving\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# here we use a flavor of gradient descent to update the weights of the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # a peculiar aspect of Pytorch -> you put the model in a \"training\" state\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "        optimizer.zero_grad()\n",
    "        # do a forward propagation\n",
    "        outputs = model(X_batch)\n",
    "        # use the criterion to compute the loss of the batch\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        # here we backpropagate to update the weigths\n",
    "        loss.backward()\n",
    "        # here, the next line is actually updating the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred)\n",
    "        y_true_list.append(y_batch)\n",
    "\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    accuracy = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "WineNet                                  --\n",
       "├─Linear: 1-1                            70\n",
       "├─PReLU: 1-2                             1\n",
       "├─Linear: 1-3                            18\n",
       "=================================================================\n",
       "Total params: 89\n",
       "Trainable params: 89\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abSQUWCW4zsi"
   },
   "source": [
    "### Example of Classification w/ Class Imbalance\n",
    "\n",
    "Reference: https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/04.first-neural-network.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLojkU_2GXsC"
   },
   "source": [
    "### Example of a Neural Net in PyTorch for Regression Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://github.com/dvasiliu/AML/blob/main/Data%20Sets/concrete.csv?raw=true')\n",
    "y = data['strength'].values\n",
    "x = data.drop(columns=['strength']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgZpQUz0GVTZ",
    "outputId": "c80f1819-bba7-479d-ebc2-aba0e4e7a81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 30.1968\n",
      "Epoch [20/100], Loss: 36.9732\n",
      "Epoch [30/100], Loss: 21.5745\n",
      "Epoch [40/100], Loss: 29.1898\n",
      "Epoch [50/100], Loss: 16.2504\n",
      "Epoch [60/100], Loss: 23.6800\n",
      "Epoch [70/100], Loss: 27.8644\n",
      "Epoch [80/100], Loss: 70.0342\n",
      "Epoch [90/100], Loss: 8.8895\n",
      "Epoch [100/100], Loss: 11.6931\n",
      "Test Loss: 35.7237\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic regression data\n",
    "#X, y = make_regression(n_samples=1000, n_features=12, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=301)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float64)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float64).view(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float64).view(-1, 1)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "# Define a simple neural network for regression\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self,n_features):\n",
    "        super(RegressorNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_features, 16).double()\n",
    "        self.a1 = nn.PReLU().double()\n",
    "        self.layer2 = nn.Linear(16, 8).double()\n",
    "        self.a2 = nn.PReLU().double()\n",
    "        self.layer3 = nn.Linear(8, 1).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.layer1(x))\n",
    "        x = self.a2(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = RegressorNN(x.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "        optimizer.zero_grad()\n",
    "        # do a forward propagation\n",
    "        outputs = model(X_batch)\n",
    "        # use the criterion to compute the loss of the batch\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        # here we backpropagate to update the weigths\n",
    "        loss.backward()\n",
    "        # here, the next line is actually updating the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Sample prediction\n",
    "# sample_data = torch.tensor(scaler.transform(np.array([[1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0, 0.1,0.4,0.8]])), dtype=torch.float32)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     sample_prediction = model(sample_data)\n",
    "#     print(f'Sample Prediction: {sample_prediction.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXlOgFMM2jWN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

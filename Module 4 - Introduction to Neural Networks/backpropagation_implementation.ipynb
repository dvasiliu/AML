{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Implementation from Scratch\n",
    "## Neural Network for XOR Problem\n",
    "\n",
    "This notebook implements a simple 2-layer neural network with backpropagation to solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Class\n",
    "\n",
    "Implements a 2-layer neural network with sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        # Output layer\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MSE Loss Class\n",
    "\n",
    "Mean Squared Error loss with backpropagation method to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.gradients = {}\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def backpropagate(self, network, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        # Output layer gradient\n",
    "        delta2 = -(y_true - network.a2) * network.sigmoid_derivative(network.a2)\n",
    "        # Gradients for W2 and b2\n",
    "        self.gradients['dW2'] = (network.a1.T @ delta2) / m\n",
    "        self.gradients['db2'] = np.sum(delta2, axis=0, keepdims=True) / m\n",
    "        # Hidden layer gradient\n",
    "        delta1 = (delta2 @ network.W2.T) * network.sigmoid_derivative(network.a1)\n",
    "        # Gradients for W1 and b1\n",
    "        self.gradients['dW1'] = (X.T @ delta1) / m\n",
    "        self.gradients['db1'] = np.sum(delta1, axis=0, keepdims=True) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SGD Optimizer Class\n",
    "\n",
    "Stochastic Gradient Descent optimizer that updates network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer: # this is fake SGD (not stochastic for this simplistic data)\n",
    "    def __init__(self, learning_rate=0.5):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def step(self, network, gradients):\n",
    "        # Update weights and biases\n",
    "        network.W2 -= self.lr * gradients['dW2']\n",
    "        network.b2 -= self.lr * gradients['db2']\n",
    "        network.W1 -= self.lr * gradients['dW1']\n",
    "        network.b1 -= self.lr * gradients['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare XOR Dataset\n",
    "\n",
    "The XOR problem is a classic test for neural networks because it's not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Truth Table:\n",
      "------------------------------\n",
      "Input: [0 0] --> Expected Output: 0\n",
      "Input: [0 1] --> Expected Output: 1\n",
      "Input: [1 0] --> Expected Output: 1\n",
      "Input: [1 1] --> Expected Output: 0\n"
     ]
    }
   ],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} --> Expected Output: {y[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Network, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized with:\n",
      "  Input size: 2\n",
      "  Hidden size: 3\n",
      "  Output size: 1\n",
      "  Learning rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize network, loss, and optimizer\n",
    "network = NeuralNetwork(input_size=2, hidden_size=3, output_size=1)\n",
    "criterion = MSELoss()\n",
    "optimizer = SGDOptimizer(learning_rate=0.5)\n",
    "\n",
    "print(\"Network initialized with:\")\n",
    "print(f\"  Input size: 2\")\n",
    "print(f\"  Hidden size: 3\")\n",
    "print(f\"  Output size: 1\")\n",
    "print(f\"  Learning rate: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Train the network for 10,000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network on XOR Problem\n",
      "==================================================\n",
      "Epoch  2000, Loss: 0.124454\n",
      "Epoch  4000, Loss: 0.067873\n",
      "Epoch  6000, Loss: 0.006640\n",
      "Epoch  8000, Loss: 0.002464\n",
      "Epoch 10000, Loss: 0.001420\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Neural Network on XOR Problem\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    predictions = network.forward(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass. This part is only computing the gradients\n",
    "    criterion.backpropagate(network, X, y)\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step(network, criterion.gradients)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 2000 == 0:\n",
    "        print(f\"Epoch {epoch + 1:5d}, Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "--------------------------------------------------\n",
      "Input: [0 0] --> Output: 0.0315 (Expected: 0)\n",
      "Input: [0 1] --> Output: 0.9529 (Expected: 1)\n",
      "Input: [1 0] --> Output: 0.9374 (Expected: 1)\n",
      "Input: [1 1] --> Output: 0.0650 (Expected: 0)\n",
      "\n",
      "==================================================\n",
      "Interpretation:\n",
      "--------------------------------------------------\n",
      "Values close to 0.0 represent False/0\n",
      "Values close to 1.0 represent True/1\n",
      "\n",
      "The network has successfully learned the XOR function!\n"
     ]
    }
   ],
   "source": [
    "# Final predictions\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} --> Output: {predictions[i][0]:.4f} (Expected: {y[i][0]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Interpretation:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Values close to 0.0 represent False/0\")\n",
    "print(\"Values close to 1.0 represent True/1\")\n",
    "print(\"\\nThe network has successfully learned the XOR function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Get the Network Weights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Weights:\n",
      "==================================================\n",
      "\n",
      "W1 (Input --> Hidden):\n",
      "[[ 6.2385695  -0.69638346  4.28320814]\n",
      " [ 6.22016816 -0.53386542  4.28597905]]\n",
      "\n",
      "b1 (Hidden bias):\n",
      "[[-2.68924575 -0.01274379 -6.5648958 ]]\n",
      "\n",
      "W2 (Hidden --> Output):\n",
      "[[ 8.84436629]\n",
      " [ 0.15700147]\n",
      " [-9.40872799]]\n",
      "\n",
      "b2 (Output bias):\n",
      "[[-4.15376572]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLearned Weights:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nW1 (Input --> Hidden):\")\n",
    "print(network.W1)\n",
    "print(\"\\nb1 (Hidden bias):\")\n",
    "print(network.b1)\n",
    "print(\"\\nW2 (Hidden --> Output):\")\n",
    "print(network.W2)\n",
    "print(\"\\nb2 (Output bias):\")\n",
    "print(network.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Example with Parametric Rectified Linear Unit Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x,y,test_size=0.2):\n",
    "    ind = np.random.permutation(range(len(x)))\n",
    "    bins = np.array_split(ind,1//test_size)\n",
    "    testidx = bins[0]\n",
    "    trainidx = np.delete(range(len(x)),testidx)\n",
    "    return x[trainidx], x[testidx], y[trainidx], y[testidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Neural Network for Concrete Compressive Strength Prediction\n",
      "Using PReLU Activation Function\n",
      "======================================================================\n",
      "\n",
      "Loading Concrete Compressive Strength Dataset...\n",
      "Dataset shape: X=(1030, 8), y=(1030, 1)\n",
      "Target statistics: mean=35.82, std=16.70 MPa\n",
      "\n",
      "Train set: 772 samples\n",
      "Test set: 258 samples\n",
      "\n",
      "Data normalized using StandardScaler (features only)\n",
      "\n",
      "Network Architecture:\n",
      "  Input layer:  8 neurons\n",
      "  Hidden layer: 20 neurons (PReLU activation)\n",
      "  Output layer: 1 neuron (Linear activation)\n",
      "  Total parameters: 221\n",
      "\n",
      "Training Started...\n",
      "----------------------------------------------------------------------\n",
      "Epoch   500 | Train Loss: 39.3940 | Test Loss: 58.9851\n",
      "Epoch  1000 | Train Loss: 29.7349 | Test Loss: 49.6617\n",
      "Epoch  1500 | Train Loss: 23.6967 | Test Loss: 43.3676\n",
      "Epoch  2000 | Train Loss: 21.9393 | Test Loss: 42.0992\n",
      "Epoch  2500 | Train Loss: 21.1856 | Test Loss: 41.9581\n",
      "Epoch  3000 | Train Loss: 20.6560 | Test Loss: 41.8888\n",
      "Epoch  3500 | Train Loss: 20.5149 | Test Loss: 42.1711\n",
      "Epoch  4000 | Train Loss: 20.8838 | Test Loss: 42.8669\n",
      "Epoch  4500 | Train Loss: 19.4537 | Test Loss: 40.8107\n",
      "Epoch  5000 | Train Loss: 19.6137 | Test Loss: 40.9638\n",
      "----------------------------------------------------------------------\n",
      "Training Complete!\n",
      "\n",
      "======================================================================\n",
      "Model Evaluation on Test Set\n",
      "======================================================================\n",
      "\n",
      "Mean Squared Error (MSE):  40.9638\n",
      "Root Mean Squared Error (RMSE): 6.4003 MPa\n",
      "Mean Absolute Error (MAE): 4.6774 MPa\n",
      "R2 Score: 0.8487\n",
      "\n",
      "======================================================================\n",
      "Learned PReLU Alpha Parameters\n",
      "======================================================================\n",
      "\n",
      "Alpha values (one per hidden neuron):\n",
      "[[ 3.43407177 -1.47592387  6.77070063 -0.5341347  -0.69778222 -1.21058515\n",
      "  -2.30020258  0.38065691 -0.70896255 -1.07030828 -1.23539655 -1.64407049\n",
      "  -0.18794244  0.58943158 -0.22938506 -1.10969386  3.16749925 -0.73549264\n",
      "  -0.80986549 -1.82425931]]\n",
      "\n",
      "Mean alpha: -0.0716\n",
      "Std alpha:  2.1153\n",
      "\n",
      "======================================================================\n",
      "Sample Predictions\n",
      "======================================================================\n",
      "\n",
      "Predicted    Actual       Error       \n",
      "------------------------------------\n",
      "68.57        74.70        -6.13       \n",
      "11.32        9.13         2.19        \n",
      "60.83        54.77        6.06        \n",
      "23.78        24.05        -0.27       \n",
      "33.64        31.84        1.80        \n",
      "37.52        52.01        -14.49      \n",
      "40.31        33.02        7.29        \n",
      "58.38        44.42        13.96       \n",
      "27.08        25.18        1.90        \n",
      "10.75        12.45        -1.70       \n",
      "\n",
      "======================================================================\n",
      "Training Summary\n",
      "======================================================================\n",
      "\n",
      "Final Training Loss:   19.613656\n",
      "Final Test Loss: 40.963833\n",
      "Test RMSE: 6.40 MPa\n"
     ]
    }
   ],
   "source": [
    "class PReLU:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize alpha parameters (one per neuron)\n",
    "        self.alpha = np.ones((1, input_size)) * 0.25\n",
    "        \n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        return np.where(z > 0, z, self.alpha * z)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Gradient w.r.t input\n",
    "        grad_z = np.where(self.z > 0, 1, self.alpha) * grad_output\n",
    "        \n",
    "        # Gradient w.r.t alpha (for updating learnable parameter)\n",
    "        grad_alpha = np.where(self.z > 0, 0, self.z) * grad_output\n",
    "        grad_alpha = np.sum(grad_alpha, axis=0, keepdims=True)\n",
    "        \n",
    "        return grad_z, grad_alpha\n",
    "\n",
    "\n",
    "class NeuralNetworkPReLU:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with He initialization (good for ReLU-like activations)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # PReLU activation for hidden layer\n",
    "        # note that this is an object based on the PReLU\n",
    "        self.prelu = PReLU(hidden_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Hidden layer with PReLU\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.prelu.forward(self.z1)\n",
    "        \n",
    "        # Output layer (linear activation for regression)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.z2  # No activation function for regression output\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.gradients = {}\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def backpropagate(self, network, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient (derivative of MSE + linear activation)\n",
    "        delta2 = (network.a2 - y_true) / m\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        self.gradients['dW2'] = network.a1.T @ delta2\n",
    "        self.gradients['db2'] = np.sum(delta2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagate through PReLU\n",
    "        delta1_pre = delta2 @ network.W2.T\n",
    "        delta1, grad_alpha = network.prelu.backward(delta1_pre)\n",
    "        \n",
    "        # Gradients for hidden layer\n",
    "        self.gradients['dW1'] = X.T @ delta1\n",
    "        self.gradients['db1'] = np.sum(delta1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient for PReLU alpha parameter\n",
    "        self.gradients['d_alpha'] = grad_alpha\n",
    "\n",
    "\n",
    "class SGDOptimizer: # (Again this is not stochastic)\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def step(self, network, gradients):\n",
    "        network.W2 -= self.lr * gradients['dW2']\n",
    "        network.b2 -= self.lr * gradients['db2']\n",
    "        network.W1 -= self.lr * gradients['dW1']\n",
    "        network.b1 -= self.lr * gradients['db1']\n",
    "        \n",
    "        # Update PReLU alpha parameter\n",
    "        network.prelu.alpha -= self.lr * gradients['d_alpha']\n",
    "\n",
    "\n",
    "def load_concrete_data():\n",
    "\n",
    "    url = \"https://github.com/dvasiliu/AML/blob/main/Data%20Sets/concrete.csv?raw=true\"\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1:].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_network(network, criterion, optimizer, X_train, y_train, \n",
    "                  X_test, y_test, epochs=6000, print_every=500):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = network.forward(X_train)\n",
    "        train_loss = criterion(predictions, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        criterion.backpropagate(network, X_train, y_train)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step(network, criterion.gradients)\n",
    "        \n",
    "        # Test loss\n",
    "        test_predictions = network.predict(X_test)\n",
    "        test_loss = criterion(test_predictions, y_test)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch + 1:5d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # R-squared\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Neural Network for Concrete Compressive Strength Prediction\")\n",
    "print(\"Using PReLU Activation Function\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Load data\n",
    "print(\"Loading Concrete Compressive Strength Dataset...\")\n",
    "X, y = load_concrete_data()\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Target statistics: mean={y.mean():.2f}, std={y.std():.2f} MPa\")\n",
    "print()\n",
    "\n",
    "# Split data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Normalize features only (NOT target values)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "print(\"Data normalized using StandardScaler (features only)\")\n",
    "print()\n",
    "\n",
    "# Initialize network\n",
    "input_size = X_train.shape[1]  # 8 features\n",
    "hidden_size = 20  # Number of neurons in hidden layer\n",
    "output_size = 1   # Single output for regression\n",
    "\n",
    "network = NeuralNetworkPReLU(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print(f\"Network Architecture:\")\n",
    "print(f\"  Input layer:  {input_size} neurons\")\n",
    "print(f\"  Hidden layer: {hidden_size} neurons (PReLU activation)\")\n",
    "print(f\"  Output layer: {output_size} neuron (Linear activation)\")\n",
    "print(f\"  Total parameters: {input_size * hidden_size + hidden_size + hidden_size * output_size + output_size + hidden_size}\")\n",
    "print()\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = MSELoss()\n",
    "optimizer = SGDOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Train the network\n",
    "print(\"Training Started...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "train_losses, test_losses = train_network(\n",
    "    network, criterion, optimizer,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    epochs=5000,\n",
    "    print_every=500\n",
    ")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Training Complete!\")\n",
    "print()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"=\" * 70)\n",
    "print(\"Model Evaluation on Test Set\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "test_predictions = network.predict(X_test)\n",
    "\n",
    "metrics = calculate_metrics(y_test, test_predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE):  {metrics['MSE']:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {metrics['RMSE']:.4f} MPa\")\n",
    "print(f\"Mean Absolute Error (MAE): {metrics['MAE']:.4f} MPa\")\n",
    "print(f\"R2 Score: {metrics['R2']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Show learned PReLU alpha values\n",
    "print(\"=\" * 70)\n",
    "print(\"Learned PReLU Alpha Parameters\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Alpha values (one per hidden neuron):\")\n",
    "print(network.prelu.alpha)\n",
    "print(f\"\\nMean alpha: {network.prelu.alpha.mean():.4f}\")\n",
    "print(f\"Std alpha:  {network.prelu.alpha.std():.4f}\")\n",
    "print()\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"=\" * 70)\n",
    "print(\"Sample Predictions\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "n_samples = min(10, len(X_test))\n",
    "print(f\"{'Predicted':<12} {'Actual':<12} {'Error':<12}\")\n",
    "print(\"-\" * 36)\n",
    "for i in range(n_samples):\n",
    "    pred = test_predictions[i, 0]\n",
    "    actual = y_test[i, 0]\n",
    "    error = pred - actual\n",
    "    print(f\"{pred:<12.2f} {actual:<12.2f} {error:<12.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Final Training Loss:   {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.6f}\")\n",
    "print(f\"Test RMSE: {metrics['RMSE']:.2f} MPa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

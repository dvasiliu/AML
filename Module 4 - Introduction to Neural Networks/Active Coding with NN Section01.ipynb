{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bbe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba59e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the wine/cultivar dataset\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24916e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  ...   hue  od280/od315_of_diluted_wines  proline\n",
       "0      14.23        1.71  2.43  ...  1.04                          3.92   1065.0\n",
       "1      13.20        1.78  2.14  ...  1.05                          3.40   1050.0\n",
       "2      13.16        2.36  2.67  ...  1.03                          3.17   1185.0\n",
       "3      14.37        1.95  2.50  ...  0.86                          3.45   1480.0\n",
       "4      13.24        2.59  2.87  ...  1.04                          2.93    735.0\n",
       "..       ...         ...   ...  ...   ...                           ...      ...\n",
       "173    13.71        5.65  2.45  ...  0.64                          1.74    740.0\n",
       "174    13.40        3.91  2.48  ...  0.70                          1.56    750.0\n",
       "175    13.27        4.28  2.26  ...  0.59                          1.56    835.0\n",
       "176    13.17        2.59  2.37  ...  0.60                          1.62    840.0\n",
       "177    14.13        4.10  2.74  ...  0.61                          1.60    560.0\n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=X,columns=wine_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f2b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=0.35,shuffle=True,random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7767682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps a good idea is to scale the input features\n",
    "scaler = StandardScaler()\n",
    "xtrain_scaled = scaler.fit_transform(xtrain)\n",
    "xtest_scaled = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0598a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(xtrain_scaled, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(xtest_scaled, dtype=torch.float64)\n",
    "y_train_tensor = torch.tensor(ytrain, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(ytest, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d130736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class MyFirstNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(MyFirstNet, self).__init__()\n",
    "        # let's define the topology first\n",
    "        self.layer1 = nn.Linear(n_features, 40).double()\n",
    "        self.layer2 = nn.Linear(40, 30).double()\n",
    "        self.layer3 = nn.Linear(30,20).double()\n",
    "        self.layer_out = nn.Linear(20,3).double()\n",
    "        # let's define the activations for each layer\n",
    "        self.a1 = nn.PReLU().double()\n",
    "        self.a2 = nn.PReLU().double()\n",
    "        self.a3 = nn.PReLU().double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.layer1(x))  # Apply PReLU activation\n",
    "        x = self.a2(self.layer2(x))\n",
    "        x = self.a3(self.layer3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MyFirstNet(xtrain.shape[1])\n",
    "# here we also choose what loss function to optimize for the machine learning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8390d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.0000\n",
      "Epoch [20/20], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    # a peculiar aspect of Pytorch -> you put the model in a \"training\" state\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "        optimizer.zero_grad()\n",
    "        # do a forward propagation\n",
    "        outputs = model(X_batch)\n",
    "        # use the criterion to compute the loss of the batch\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        # here we backpropagate to update the weigths\n",
    "        loss.backward()# the approximation of the gradient \n",
    "        optimizer.step() # the actual update of weights with the algorithm chosen\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "868e63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # always have tracker avriables for what you need\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred)\n",
    "        y_true_list.append(y_batch)\n",
    "\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    accuracy = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "    print(f'Accuracy on train set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f10b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # always have tracker variables for what you need\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred)\n",
    "        y_true_list.append(y_batch)\n",
    "\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    accuracy = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eaa3e9",
   "metadata": {},
   "source": [
    "## Stratified KFold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f51ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skf_evaluation(x,y,k=5):\n",
    "    skf = StratifiedKFold(n_splits=k,shuffle=True,random_state=301)\n",
    "    acc = []\n",
    "    for j, (idxtrain, idxtest) in enumerate(skf.split(x,y)):\n",
    "        xtrain_scaled = scaler.fit_transform(x[idxtrain])\n",
    "        xtest_scaled = scaler.transform(x[idxtest])\n",
    "        ytrain = y[idxtrain]\n",
    "        ytest = y[idxtest]\n",
    "        X_train_tensor = torch.tensor(xtrain_scaled, dtype=torch.float64)\n",
    "        X_test_tensor = torch.tensor(xtest_scaled, dtype=torch.float64)\n",
    "        y_train_tensor = torch.tensor(ytrain, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(ytest, dtype=torch.long)\n",
    "\n",
    "        # Create DataLoader for training and testing\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "        model = MyFirstNet(xtrain.shape[1])\n",
    "        # here we also choose what loss function to optimize for the machine learning\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # now train the model on the train data and collect the accuracy on the test\n",
    "        # Train the model\n",
    "        num_epochs = 20\n",
    "        for epoch in range(num_epochs):\n",
    "            # a peculiar aspect of Pytorch -> you put the model in a \"training\" state\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "                optimizer.zero_grad()\n",
    "                # do a forward propagation\n",
    "                outputs = model(X_batch)\n",
    "                # use the criterion to compute the loss of the batch\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # here we backpropagate to update the weigths\n",
    "                loss.backward()# the approximation of the gradient \n",
    "                optimizer.step() # the actual update of weights with the algorithm chosen\n",
    "                # if (epoch+1) % 10 == 0:\n",
    "                #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # use the trained model to predict on the test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # always have tracker variables for what you need\n",
    "            y_pred_list = []\n",
    "            y_true_list = []\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                _, y_pred = torch.max(outputs, 1)\n",
    "                y_pred_list.append(y_pred)\n",
    "                y_true_list.append(y_batch)\n",
    "\n",
    "            y_pred = torch.cat(y_pred_list)\n",
    "            y_true = torch.cat(y_true_list)\n",
    "            accuracy = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "            acc.append(accuracy)\n",
    "            print(f'Accuracy on test set #{j}: {accuracy:.4f}')\n",
    "    return np.mean(acc)\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d96f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set #0: 0.9722\n",
      "Accuracy on test set #1: 0.9722\n",
      "Accuracy on test set #2: 0.9722\n",
      "Accuracy on test set #3: 1.0000\n",
      "Accuracy on test set #4: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9833333333333332)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf_evaluation(X,y,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330752",
   "metadata": {},
   "source": [
    "## A Regression Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb4e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/dvasiliu/AML/blob/main/Data%20Sets/housing.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c835c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>town</th>\n",
       "      <th>tract</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>crime</th>\n",
       "      <th>residential</th>\n",
       "      <th>industrial</th>\n",
       "      <th>river</th>\n",
       "      <th>nox</th>\n",
       "      <th>rooms</th>\n",
       "      <th>older</th>\n",
       "      <th>distance</th>\n",
       "      <th>highway</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>cmedv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nahant</td>\n",
       "      <td>2011</td>\n",
       "      <td>-70.955002</td>\n",
       "      <td>42.255001</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>no</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.199997</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swampscott</td>\n",
       "      <td>2021</td>\n",
       "      <td>-70.949997</td>\n",
       "      <td>42.287498</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>no</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Swampscott</td>\n",
       "      <td>2022</td>\n",
       "      <td>-70.935997</td>\n",
       "      <td>42.283001</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>no</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.099998</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marblehead</td>\n",
       "      <td>2031</td>\n",
       "      <td>-70.928001</td>\n",
       "      <td>42.292999</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>no</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.400002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marblehead</td>\n",
       "      <td>2032</td>\n",
       "      <td>-70.921997</td>\n",
       "      <td>42.298000</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>no</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Winthrop</td>\n",
       "      <td>1801</td>\n",
       "      <td>-70.986000</td>\n",
       "      <td>42.231201</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>no</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.099998</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Winthrop</td>\n",
       "      <td>1802</td>\n",
       "      <td>-70.990997</td>\n",
       "      <td>42.227501</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>no</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.699997</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Winthrop</td>\n",
       "      <td>1803</td>\n",
       "      <td>-70.994797</td>\n",
       "      <td>42.226002</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>no</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Winthrop</td>\n",
       "      <td>1804</td>\n",
       "      <td>-70.987503</td>\n",
       "      <td>42.223999</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>no</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.300003</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Winthrop</td>\n",
       "      <td>1805</td>\n",
       "      <td>-70.982498</td>\n",
       "      <td>42.221001</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>no</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.800003</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.88</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           town  tract  longitude   latitude  ...  tax    ptratio  lstat      cmedv\n",
       "0        Nahant   2011 -70.955002  42.255001  ...  296  15.300000   4.98  24.000000\n",
       "1    Swampscott   2021 -70.949997  42.287498  ...  242  17.799999   9.14  21.600000\n",
       "2    Swampscott   2022 -70.935997  42.283001  ...  242  17.799999   4.03  34.700001\n",
       "3    Marblehead   2031 -70.928001  42.292999  ...  222  18.700001   2.94  33.400002\n",
       "4    Marblehead   2032 -70.921997  42.298000  ...  222  18.700001   5.33  36.200001\n",
       "..          ...    ...        ...        ...  ...  ...        ...    ...        ...\n",
       "501    Winthrop   1801 -70.986000  42.231201  ...  273  21.000000   9.67  22.400000\n",
       "502    Winthrop   1802 -70.990997  42.227501  ...  273  21.000000   9.08  20.600000\n",
       "503    Winthrop   1803 -70.994797  42.226002  ...  273  21.000000   5.64  23.900000\n",
       "504    Winthrop   1804 -70.987503  42.223999  ...  273  21.000000   6.48  22.000000\n",
       "505    Winthrop   1805 -70.982498  42.221001  ...  273  21.000000   7.88  19.000000\n",
       "\n",
       "[506 rows x 17 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947d5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retain just the numerical variables do this\n",
    "x = data.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
    "y = data['cmedv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e65684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnet_regression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(neuralnet_regression, self).__init__()\n",
    "        # let's define the topology first\n",
    "        self.layer1 = nn.Linear(n_features, 30).double()\n",
    "        self.layer2 = nn.Linear(30, 20).double()\n",
    "        self.layer3 = nn.Linear(10,5).double()\n",
    "        self.layer_out = nn.Linear(20,1).double()\n",
    "        # let's define the activations for each layer\n",
    "        self.a1 = nn.PReLU().double()\n",
    "        self.a2 = nn.PReLU().double()\n",
    "        self.a3 = nn.GELU().double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.layer1(x))  # Apply PReLU activation\n",
    "        x = self.a2(self.layer2(x))\n",
    "        #x = self.a3(self.layer3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "    def elastic_net_regularization(self, alpha=0.01, l1_ratio=0.5):\n",
    "\n",
    "      l1_reg = 0\n",
    "      l2_reg = 0\n",
    "\n",
    "      # Apply regularization to weights in all linear layers\n",
    "      for name, param in self.named_parameters():\n",
    "          if 'weight' in name:\n",
    "              l1_reg += torch.norm(param, 1).double()  # L1 norm (sum of absolute values)\n",
    "              l2_reg += torch.norm(param, 2).double() ** 2  # L2 norm squared (sum of squares)\n",
    "\n",
    "      # Combine L1 and L2 regularization\n",
    "      reg_loss = alpha *(l1_ratio * l1_reg + 0.5*(1-l1_ratio) * l2_reg)\n",
    "\n",
    "      return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d863168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuralnet_regression(x.shape[1])\n",
    "criterion = nn.MSELoss().double()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57b80250",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.35,shuffle=True,random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b70f46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps a good idea is to scale the input features\n",
    "scaler = StandardScaler()\n",
    "xtrain_scaled = scaler.fit_transform(xtrain)\n",
    "xtest_scaled = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7b82e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(xtrain_scaled, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(xtest_scaled, dtype=torch.float64)\n",
    "y_train_tensor = torch.tensor(ytrain, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(ytest, dtype=torch.float64)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203a52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b7cf459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvasiliu/work_p13/lib64/python3.13/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss (MSE): 370.6787\n",
      "Epoch [20/100], Loss (MSE): 820.9691\n",
      "Epoch [30/100], Loss (MSE): 771.1392\n",
      "Epoch [40/100], Loss (MSE): 942.0142\n",
      "Epoch [50/100], Loss (MSE): 529.2059\n",
      "Epoch [60/100], Loss (MSE): 577.5363\n",
      "Epoch [70/100], Loss (MSE): 939.5683\n",
      "Epoch [80/100], Loss (MSE): 622.3169\n",
      "Epoch [90/100], Loss (MSE): 571.3141\n",
      "Epoch [100/100], Loss (MSE): 552.8695\n"
     ]
    }
   ],
   "source": [
    "model = neuralnet_regression(x.shape[1])\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # a peculiar aspect of Pytorch -> you put the model in a \"training\" state\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # this resets the optimizer before each calculation of the direction for updating the weights\n",
    "        optimizer.zero_grad()\n",
    "        # do a forward propagation\n",
    "        outputs = model(X_batch)\n",
    "        # use the criterion to compute the loss of the batch\n",
    "        main_loss = criterion(outputs, y_batch)\n",
    "        reg_loss = model.elastic_net_regularization(alpha=0.02,l1_ratio=0.5)\n",
    "        loss = main_loss + reg_loss\n",
    "        # here we backpropagate to update the weigths\n",
    "        loss.backward()# the approximation of the gradient \n",
    "        optimizer.step() # the actual update of weights with the algorithm chosen\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss (MSE): {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb39a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_p13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
